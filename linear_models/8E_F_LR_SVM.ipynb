{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h43kDT3M41u5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 5) (1000, 5) (1000, 5)\n"
     ]
    }
   ],
   "source": [
    "# you can write your code here\n",
    "# Split the data into  ùëãùë°ùëüùëéùëñùëõ (60),  ùëãùëêùë£ (20),  ùëãùë°ùëíùë†ùë° (20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr,X_test,y_tr,y_test=train_test_split(X,y,test_size=0.2)\n",
    "X_train,X_cv,y_train,y_cv=train_test_split(X_tr,y_tr,test_size=0.25)\n",
    "print(X_train.shape,X_test.shape,X_cv.shape)\n",
    "\n",
    "clf=SVC(ùëîùëéùëöùëöùëé=0.001,ùê∂=100)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "f_cv=clf.decision_function(X_cv)\n",
    "\n",
    "\n",
    "\n",
    "intercept=clf.intercept_\n",
    "dual_coef=clf.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 539), (539, 5))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dual_coef.shape,clf.support_vectors_.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:04<00:00, 241.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom decision function output = [ 1.61371758  2.60412744 -3.24594487 ...  0.685486   -2.75966952\n",
      "  1.42570403]\n",
      "Built-in decision function output = 1.6137175818389424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inter=clf.intercept_\n",
    "sup_vecs=clf.support_vectors_\n",
    "dual_coef=clf.dual_coef_\n",
    "from tqdm import tqdm\n",
    "g=0.001\n",
    "ans=0\n",
    "for Xq in tqdm(X_cv):\n",
    "    for i in range(np.shape(sup_vecs)[0]):\n",
    "        k=np.linalg.norm(sup_vecs[i,:]-Xq)\n",
    "        ans =+  dual_coef[0,i]  *  np.exp(-g*(k**2))\n",
    "    ans =+ inter  \n",
    "    fcv=np.append(f_cv,ans)\n",
    "\n",
    "print(\"Custom decision function output =\",fcv)\n",
    "head, *tail= f_cv\n",
    "print(\"Built-in decision function output =\",head)\n",
    "#print(dual_coef[0,0],sup_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-105d68afb808>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#match_count(x1,x2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mN_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_cv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mN_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_cv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0my_n\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_n\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_cv' is not defined"
     ]
    }
   ],
   "source": [
    "def convter(data):\n",
    "    x=[]\n",
    "    for i in data:\n",
    "        if i<=0:         \n",
    "            y=(1/(i+2))          # y- =1/(N-)+2\n",
    "            x=np.append(x,y)\n",
    "        else:\n",
    "            y=(i+1)/(i+2)        # y+ =(N+)+1/(N+)+2\n",
    "            x=np.append(x,y)\n",
    "    return x     \n",
    "\n",
    "def match_count(x1,x2):\n",
    "    count=0\n",
    "    for i in range(1000):\n",
    "        if x1[i]!=x2[i]:\n",
    "            count=count+1\n",
    "    print(count)\n",
    "    \n",
    "#x1=convter(y_cv)\n",
    "#x2=convter(f_cv)\n",
    "#match_count(x1,x2)\n",
    "\n",
    "N_p=list(y_cv).count(1)\n",
    "N_n=list(y_cv).count(0)\n",
    "y_n=(1/(N_n+2))\n",
    "y_p=((N_n+1)/(N_n+2))\n",
    "\n",
    "x1=x1.reshape(1,-1)\n",
    "x2=x2.reshape(1,-1)\n",
    "y_p,y_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(row_vector):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    w=np.zeros_like(row_vector)\n",
    "    b=0\n",
    "    return w,b\n",
    "\n",
    "def sigmoid(z):\n",
    "    s=1/(1+(np.exp(-z)))\n",
    "    return s\\\n",
    "\n",
    "def logloss(y_true,y_pred):\n",
    "    sum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        sum +=(y_true[i] * np.log10(y_pred[i])) + ((1-y_true[i]) * np.log10(1-y_pred[i]))\n",
    "    loss = -1 * (1 / len(y_true)) * sum\n",
    "    return np.round(loss,8)\n",
    "\n",
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    dw=(x * (y-sigmoid(np.dot(w.T,x)+b)) )-((alpha/N)* w)\n",
    "    return np.round(dw,7)\n",
    "\n",
    "def gradient_db(x,y,w,b):\n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "    db = y-sigmoid(np.dot(w.T,x)+b)\n",
    "    return np.round(db,6)\n",
    "\n",
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        predict.append(sigmoid(z))\n",
    "    return np.array(predict)\n",
    "\n",
    "from tqdm import tqdm\n",
    "def train(X_train,y_train,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    w,b = initialize_weights(X_train[0])\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i in range(len(X_train)):\n",
    "            dw=gradient_dw(X_train[i],y_train[i],w,b,alpha,N)\n",
    "            db=gradient_db(X_train[i],y_train[i],w,b)\n",
    "            w= w + (alpha * dw)\n",
    "            b= b + (alpha * db)\n",
    "            \n",
    "        y_pred_train=pred(w,b,X_train)\n",
    "        y_pred_train=np.clip(y_pred_train, 1e-15, 0.9999999)    \n",
    "        loss1=logloss(y_train,y_pred_train)\n",
    "        train_loss.append(loss1)   \n",
    "    \n",
    "    return w,b,train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 2592.28it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha=0.001\n",
    "eta0=0.001\n",
    "N=len(x1)\n",
    "epochs=20\n",
    "w1,b1,train_loss1 = train(x1 , x2 , epochs , alpha , eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test  = clf.decision_function( X_test )\n",
    "\n",
    "#print(w1,b1,train_loss1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.98559018e-01, 4.95792984e-01, 4.90900122e-01, 4.98896922e-01,\n",
       "       5.07066374e-01, 5.00065169e-01, 2.73856093e-01, 4.92490827e-01,\n",
       "       5.04931821e-01, 4.90558189e-01, 4.99357007e-01, 4.99923074e-01,\n",
       "       4.93002975e-01, 4.63033106e-01, 4.96225029e-01, 4.96750470e-01,\n",
       "       4.99873053e-01, 5.00694638e-01, 5.05434356e-01, 4.93729748e-01,\n",
       "       5.20078292e-01, 4.99515262e-01, 5.02420902e-01, 4.88162603e-01,\n",
       "       4.99363027e-01, 4.93109227e-01, 5.00767795e-01, 4.99566378e-01,\n",
       "       3.68153489e-01, 4.98997408e-01, 4.96151278e-01, 5.04316554e-01,\n",
       "       4.98843547e-01, 5.08422209e-01, 4.97435378e-01, 4.95768886e-01,\n",
       "       4.99639279e-01, 5.08609339e-01, 4.82808581e-01, 4.98361926e-01,\n",
       "       4.93283499e-01, 5.52239441e-01, 5.01657890e-01, 5.04315598e-01,\n",
       "       5.23366588e-01, 5.28887831e-01, 5.00763341e-01, 5.05693953e-01,\n",
       "       5.00976908e-01, 4.98730791e-01, 5.01297948e-01, 4.87937582e-01,\n",
       "       4.98494063e-01, 4.97841866e-01, 5.01057263e-01, 5.03823304e-01,\n",
       "       5.04573524e-01, 5.02269903e-01, 4.97905725e-01, 4.98533745e-01,\n",
       "       4.96628103e-01, 4.98757956e-01, 5.04685841e-01, 5.00764729e-01,\n",
       "       4.99057493e-01, 5.00160016e-01, 5.05831246e-01, 5.02749874e-01,\n",
       "       5.06017702e-01, 5.04012152e-01, 4.99358838e-01, 4.99214862e-01,\n",
       "       4.95690452e-01, 5.03384206e-01, 4.97611677e-01, 4.93483690e-01,\n",
       "       4.83497816e-01, 5.06527077e-01, 5.01293305e-01, 4.97100853e-01,\n",
       "       4.99375496e-01, 4.97244041e-01, 4.87043871e-01, 4.97388966e-01,\n",
       "       4.99836950e-01, 4.99372390e-01, 5.07878287e-01, 4.98380126e-01,\n",
       "       4.96277824e-01, 4.98011372e-01, 5.05437972e-01, 4.92490988e-01,\n",
       "       5.03097732e-01, 5.03981290e-01, 4.84842269e-01, 4.96906943e-01,\n",
       "       5.02290239e-01, 4.91655044e-01, 5.03531036e-01, 4.97499173e-01,\n",
       "       4.97614788e-01, 5.04564154e-01, 5.03280546e-01, 4.99267464e-01,\n",
       "       4.96446571e-01, 4.98492165e-01, 5.04744366e-01, 4.93782173e-01,\n",
       "       4.97468447e-01, 4.98840849e-01, 5.26854043e-01, 4.96684137e-01,\n",
       "       4.99278121e-01, 5.00593335e-01, 4.99795643e-01, 5.02167489e-01,\n",
       "       5.09777793e-01, 4.92763492e-01, 4.85397176e-01, 5.29183771e-01,\n",
       "       4.98370629e-01, 4.83414514e-01, 4.96543348e-01, 5.14187853e-01,\n",
       "       4.97017364e-01, 4.92853137e-01, 5.00216283e-01, 5.26612296e-01,\n",
       "       4.97857368e-01, 4.98524071e-01, 4.96857841e-01, 4.95066999e-01,\n",
       "       4.95135170e-01, 3.63016618e-01, 4.84038662e-01, 4.73960896e-01,\n",
       "       4.92004115e-01, 5.00770260e-01, 5.00842392e-01, 4.99211569e-01,\n",
       "       5.16108978e-01, 4.99053949e-01, 4.95967499e-01, 4.94403236e-01,\n",
       "       5.02424677e-01, 5.09137784e-01, 5.00030850e-01, 4.98100225e-01,\n",
       "       4.99776481e-01, 5.05570424e-01, 4.97209648e-01, 4.96820909e-01,\n",
       "       4.98015642e-01, 4.99674250e-01, 5.12667634e-01, 5.30598639e-01,\n",
       "       4.94560730e-01, 4.99009319e-01, 4.97546165e-01, 4.77386355e-01,\n",
       "       4.99631461e-01, 4.97786274e-01, 4.96442822e-01, 3.74135993e-01,\n",
       "       5.01164560e-01, 5.01505046e-01, 3.78962402e-01, 5.01476887e-01,\n",
       "       5.09831322e-01, 5.21387269e-01, 5.03605340e-01, 4.97861774e-01,\n",
       "       4.82703958e-01, 4.95391358e-01, 4.99880431e-01, 5.06498860e-01,\n",
       "       4.92025949e-01, 4.96122168e-01, 5.02278350e-01, 4.90747911e-01,\n",
       "       5.02991539e-01, 5.09278844e-01, 5.00554281e-01, 4.77504588e-01,\n",
       "       4.77445114e-01, 4.05664267e-01, 5.15493568e-01, 5.04393781e-01,\n",
       "       5.00192118e-01, 4.95057394e-01, 5.02995299e-01, 4.97913734e-01,\n",
       "       5.00907361e-01, 5.19568995e-01, 4.96613772e-01, 5.00120106e-01,\n",
       "       5.05823146e-01, 4.96833944e-01, 5.01797300e-01, 5.10120708e-01,\n",
       "       4.97585183e-01, 5.39633110e-01, 4.97271285e-01, 5.05927446e-01,\n",
       "       4.97397200e-01, 5.11067186e-01, 5.01798968e-01, 5.06298743e-01,\n",
       "       4.98509661e-01, 5.07790909e-01, 4.94862195e-01, 5.02457439e-01,\n",
       "       4.99470912e-01, 4.96935242e-01, 5.02961067e-01, 4.97744696e-01,\n",
       "       4.95758166e-01, 4.87454123e-01, 4.65850329e-01, 5.06225053e-01,\n",
       "       5.01634326e-01, 4.96497159e-01, 5.04226554e-01, 5.01311658e-01,\n",
       "       4.96928113e-01, 4.91677925e-01, 4.93604178e-01, 4.97974199e-01,\n",
       "       4.94704002e-01, 4.89940983e-01, 4.99906034e-01, 5.04108168e-01,\n",
       "       5.04988457e-01, 5.00332737e-01, 4.96265420e-01, 4.96455348e-01,\n",
       "       5.00871701e-01, 3.24513464e-01, 4.99950092e-01, 5.12654037e-01,\n",
       "       4.98524808e-01, 4.97657367e-01, 4.84430577e-01, 4.97625326e-01,\n",
       "       5.01405944e-01, 4.80069853e-01, 5.02285646e-01, 4.95504606e-01,\n",
       "       5.04602519e-01, 5.00981814e-01, 5.00007847e-01, 5.01651785e-01,\n",
       "       4.74666186e-01, 4.94495251e-01, 5.06070229e-01, 5.01670896e-01,\n",
       "       4.94999450e-01, 4.99853255e-01, 5.04311765e-01, 4.97155657e-01,\n",
       "       4.99476545e-01, 5.12354104e-01, 4.92930793e-01, 5.50840384e-01,\n",
       "       4.75832501e-01, 4.95727341e-01, 5.01806459e-01, 4.91325522e-01,\n",
       "       5.01924461e-01, 4.94359109e-01, 4.98877687e-01, 5.01207141e-01,\n",
       "       4.98865778e-01, 4.93971583e-01, 5.00730137e-01, 5.06370531e-01,\n",
       "       5.10295398e-01, 4.99436702e-01, 5.05532246e-01, 4.98777908e-01,\n",
       "       6.70113126e-01, 4.99298199e-01, 4.98501093e-01, 4.84248476e-01,\n",
       "       4.96081492e-01, 4.99025138e-01, 4.77854312e-01, 4.92451489e-01,\n",
       "       4.98116626e-01, 4.96786613e-01, 4.95827102e-01, 5.00046505e-01,\n",
       "       5.01113570e-01, 5.74866041e-01, 4.85043275e-01, 4.98219253e-01,\n",
       "       4.98066498e-01, 4.98175729e-01, 4.99251541e-01, 5.03732792e-01,\n",
       "       4.85834208e-01, 4.93056327e-01, 4.98133950e-01, 5.00721298e-01,\n",
       "       5.20638815e-01, 4.96393885e-01, 4.98125381e-01, 5.03793104e-01,\n",
       "       4.97997946e-01, 4.95515149e-01, 5.04053719e-01, 5.03122590e-01,\n",
       "       4.98543550e-01, 4.97022360e-01, 5.02032354e-01, 5.17069804e-01,\n",
       "       4.99030022e-01, 5.03658419e-01, 5.04975673e-01, 4.97768495e-01,\n",
       "       4.88187564e-01, 5.03813900e-01, 5.02042513e-01, 4.98414521e-01,\n",
       "       4.99178424e-01, 4.27858047e-01, 5.03253818e-01, 4.97481134e-01,\n",
       "       4.40525061e-01, 5.10889892e-01, 5.05688414e-01, 5.08326675e-01,\n",
       "       5.03740907e-01, 4.89552621e-01, 5.00501655e-01, 4.99717537e-01,\n",
       "       4.26789615e-01, 5.09513280e-01, 4.90747333e-01, 3.87920872e-01,\n",
       "       4.99525098e-01, 5.03649697e-01, 5.02938236e-01, 4.99062708e-01,\n",
       "       4.96452966e-01, 4.81716811e-01, 5.07022908e-01, 5.02974819e-01,\n",
       "       4.96821935e-01, 5.01826911e-01, 4.99537323e-01, 5.07139116e-01,\n",
       "       4.65925917e-01, 4.91795391e-01, 4.97898526e-01, 4.99414836e-01,\n",
       "       4.99916565e-01, 4.98447086e-01, 4.94833477e-01, 4.95483068e-01,\n",
       "       5.07091675e-01, 5.03642361e-01, 4.37732981e-01, 5.02671657e-01,\n",
       "       4.95798269e-01, 4.94611630e-01, 4.86806283e-01, 4.93240138e-01,\n",
       "       4.96950373e-01, 5.02422036e-01, 5.10044354e-01, 4.98157044e-01,\n",
       "       4.96129522e-01, 4.99867150e-01, 4.85858955e-01, 5.05162615e-01,\n",
       "       5.00045881e-01, 5.02711727e-01, 4.65799179e-01, 5.00642516e-01,\n",
       "       5.01590278e-01, 4.97322336e-01, 5.57604647e-01, 5.01581615e-01,\n",
       "       4.98384736e-01, 4.95207751e-01, 5.03239214e-01, 5.00675954e-01,\n",
       "       5.02412508e-01, 4.99788230e-01, 4.99064927e-01, 4.94473000e-01,\n",
       "       5.21933786e-01, 4.99846474e-01, 4.98490642e-01, 4.95221343e-01,\n",
       "       5.34688847e-01, 5.04972301e-01, 4.93622281e-01, 4.98413855e-01,\n",
       "       4.95237808e-01, 5.02567717e-01, 4.98371813e-01, 5.00603333e-01,\n",
       "       4.73369798e-01, 5.02323289e-01, 5.01333758e-01, 5.02364099e-01,\n",
       "       4.99454398e-01, 5.12555436e-01, 5.01536024e-01, 5.01932438e-01,\n",
       "       5.03904167e-01, 4.93901006e-01, 5.10288042e-01, 5.05946288e-01,\n",
       "       5.02210962e-01, 4.94325498e-01, 5.05585770e-01, 4.96985481e-01,\n",
       "       4.96601136e-01, 4.99260314e-01, 5.03262025e-01, 5.66045321e-01,\n",
       "       4.82288518e-01, 4.95077854e-01, 4.83003022e-01, 4.78120386e-01,\n",
       "       5.05336045e-01, 4.95002805e-01, 4.97627743e-01, 5.01653396e-01,\n",
       "       4.98349185e-01, 5.01381554e-01, 4.92851166e-01, 4.97689670e-01,\n",
       "       5.02743907e-01, 5.00447593e-01, 4.98911797e-01, 5.01363373e-01,\n",
       "       5.04417756e-01, 5.04224128e-01, 5.05062219e-01, 4.93819785e-01,\n",
       "       5.09685983e-01, 5.03032881e-01, 4.94697043e-01, 5.02086607e-01,\n",
       "       5.00560370e-01, 5.21345408e-01, 4.98759706e-01, 5.05473438e-01,\n",
       "       4.95776255e-01, 4.84049372e-01, 4.87077934e-01, 5.01420362e-01,\n",
       "       5.71649432e-01, 4.92626386e-01, 5.05076589e-01, 4.99377608e-01,\n",
       "       4.99797409e-01, 5.03231647e-01, 5.00609872e-01, 4.98210719e-01,\n",
       "       5.04987984e-01, 4.88959765e-01, 4.96898267e-01, 4.98466808e-01,\n",
       "       4.83817353e-01, 4.99890486e-01, 5.04277680e-01, 4.96037761e-01,\n",
       "       5.08051831e-01, 4.96765614e-01, 5.25835440e-01, 4.99400277e-01,\n",
       "       5.12255166e-01, 5.00450285e-01, 4.99291354e-01, 4.29145961e-01,\n",
       "       4.99710630e-01, 5.03260863e-01, 5.05475656e-01, 4.87603082e-01,\n",
       "       5.17777524e-01, 5.03344852e-01, 5.79488455e-01, 4.98755084e-01,\n",
       "       5.01928693e-01, 5.13086105e-01, 4.93595925e-01, 4.96645506e-01,\n",
       "       4.97559552e-01, 4.97269568e-01, 5.01570453e-01, 4.92836130e-01,\n",
       "       5.00774726e-01, 5.06454593e-01, 5.00011602e-01, 5.01782479e-01,\n",
       "       5.03257870e-01, 4.98133476e-01, 5.20260792e-01, 5.18717725e-01,\n",
       "       5.02107249e-01, 5.01497998e-01, 5.05471107e-01, 5.09335486e-01,\n",
       "       5.49612508e-01, 4.95983773e-01, 5.00705007e-01, 4.94234449e-01,\n",
       "       5.09083204e-01, 4.95858224e-01, 4.88196162e-01, 4.99041585e-01,\n",
       "       4.99865232e-01, 5.03606052e-01, 4.97182714e-01, 5.04663487e-01,\n",
       "       5.02024739e-01, 4.86926688e-01, 5.01810278e-01, 5.05259372e-01,\n",
       "       4.99601721e-01, 4.89493970e-01, 4.97268798e-01, 5.09016514e-01,\n",
       "       5.03521787e-01, 4.92285315e-01, 5.11512662e-01, 4.90887431e-01,\n",
       "       4.98810000e-01, 5.01653853e-01, 4.96321340e-01, 4.96718359e-01,\n",
       "       5.00759353e-01, 5.00354836e-01, 5.02747454e-01, 4.98853652e-01,\n",
       "       4.97982671e-01, 4.76684822e-01, 5.18292117e-01, 5.04832035e-01,\n",
       "       4.94971546e-01, 5.00521545e-01, 1.56036008e-01, 4.91704565e-01,\n",
       "       4.21806825e-01, 4.98011102e-01, 4.92798117e-01, 5.05951742e-01,\n",
       "       5.04793373e-01, 5.01409808e-01, 4.96897176e-01, 5.02386201e-01,\n",
       "       4.97606805e-01, 5.04711697e-01, 5.07773341e-01, 4.94443098e-01,\n",
       "       4.97049484e-01, 4.95292734e-01, 5.01194932e-01, 5.06982464e-01,\n",
       "       5.01351323e-01, 5.01705432e-01, 5.01978440e-01, 5.06144142e-01,\n",
       "       5.27793769e-01, 4.78198348e-01, 4.99347867e-01, 5.05441472e-01,\n",
       "       4.97210783e-01, 4.99757949e-01, 4.97088090e-01, 5.02921453e-01,\n",
       "       5.03900362e-01, 4.98691414e-01, 5.04925669e-01, 4.98648952e-01,\n",
       "       4.90902223e-01, 4.96443285e-01, 4.71568321e-01, 4.98259811e-01,\n",
       "       4.99133924e-01, 5.04261307e-01, 4.97179250e-01, 5.01203442e-01,\n",
       "       4.98370414e-01, 5.14173208e-01, 5.16772421e-01, 4.99358651e-01,\n",
       "       4.98671761e-01, 5.04828027e-01, 4.99007161e-01, 4.99005330e-01,\n",
       "       4.98456788e-01, 4.89317212e-01, 4.90514998e-01, 5.31148102e-01,\n",
       "       4.99662889e-01, 5.06233811e-01, 5.04864573e-01, 4.98611811e-01,\n",
       "       5.03489335e-01, 5.10272722e-01, 5.00882593e-01, 5.01262844e-01,\n",
       "       4.79862840e-01, 4.85324580e-01, 4.96958182e-01, 5.08798964e-01,\n",
       "       4.72234449e-01, 5.02924769e-01, 5.04584773e-01, 4.92428344e-01,\n",
       "       4.98062431e-01, 4.13810587e-07, 5.04187430e-01, 4.98246191e-01,\n",
       "       4.97200352e-01, 4.58445190e-01, 5.03564402e-01, 5.01625543e-01,\n",
       "       5.09055265e-01, 5.04846000e-01, 4.95955396e-01, 5.09907198e-01,\n",
       "       8.47049572e-01, 4.93512167e-01, 4.85638983e-01, 4.96634947e-01,\n",
       "       5.04167095e-01, 5.00507531e-01, 5.04732039e-01, 5.03381555e-01,\n",
       "       5.02892722e-01, 4.93908945e-01, 4.97171266e-01, 5.03233741e-01,\n",
       "       4.99239827e-01, 5.00404407e-01, 4.84297561e-01, 5.04112206e-01,\n",
       "       4.95788261e-01, 4.95191252e-01, 4.97875000e-01, 5.03171421e-01,\n",
       "       4.89494359e-01, 5.00639781e-01, 4.96295116e-01, 4.94728005e-01,\n",
       "       5.05705145e-01, 4.96659438e-01, 5.00719727e-01, 5.01662410e-01,\n",
       "       5.13590877e-01, 4.99923361e-01, 4.85909769e-01, 5.03785545e-01,\n",
       "       4.92848630e-01, 5.05062877e-01, 5.07139408e-01, 5.04639992e-01,\n",
       "       4.97839382e-01, 5.14855982e-01, 4.97497802e-01, 4.70388693e-01,\n",
       "       4.99761235e-01, 4.97630607e-01, 5.01430695e-01, 5.06926088e-01,\n",
       "       4.90446773e-01, 4.89475069e-01, 5.08371782e-01, 5.00162738e-01,\n",
       "       5.00711734e-01, 4.99272872e-01, 5.00848870e-01, 4.98170785e-01,\n",
       "       5.02648226e-01, 4.99236495e-01, 5.00570165e-01, 5.02326223e-01,\n",
       "       5.03711057e-01, 4.98503713e-01, 5.02255036e-01, 4.94583504e-01,\n",
       "       5.00947676e-01, 4.97241873e-01, 5.02301826e-01, 5.02449455e-01,\n",
       "       4.96854267e-01, 5.04047381e-01, 5.03014896e-01, 5.05103072e-01,\n",
       "       5.01662664e-01, 4.94420945e-01, 5.09147355e-01, 5.06503837e-01,\n",
       "       5.03406255e-01, 5.05894375e-01, 4.94598427e-01, 5.08995767e-01,\n",
       "       5.23841061e-01, 4.99915517e-01, 5.04152211e-01, 5.02137331e-01,\n",
       "       4.99126125e-01, 4.95468662e-01, 5.03792276e-01, 4.88041979e-01,\n",
       "       4.99518920e-01, 4.91335573e-01, 4.90604717e-01, 5.05480196e-01,\n",
       "       4.99060634e-01, 4.96537497e-01, 5.03779553e-01, 5.02333530e-01,\n",
       "       5.05965296e-01, 5.00596751e-01, 5.04162959e-01, 4.96964166e-01,\n",
       "       4.90751873e-01, 5.01076608e-01, 5.04380341e-01, 4.96733231e-01,\n",
       "       4.72824156e-01, 5.03662489e-01, 5.00610813e-01, 4.99851247e-01,\n",
       "       5.00244482e-01, 4.97088461e-01, 5.02962174e-01, 5.04262029e-01,\n",
       "       5.38677304e-01, 5.03408960e-01, 4.98874100e-01, 5.00396586e-01,\n",
       "       4.05783701e-01, 4.85465116e-01, 4.29116588e-01, 4.96679085e-01,\n",
       "       5.01842438e-01, 5.03125107e-01, 4.95171011e-01, 4.86535981e-01,\n",
       "       4.72959812e-01, 4.92660738e-01, 5.04517742e-01, 5.02030917e-01,\n",
       "       5.10136950e-01, 4.97378934e-01, 5.03462359e-01, 5.06927682e-01,\n",
       "       4.97803717e-01, 1.83013505e-01, 4.97663831e-01, 4.95677121e-01,\n",
       "       5.10840159e-01, 4.96228243e-01, 5.05461620e-01, 5.30372913e-01,\n",
       "       4.98837802e-01, 4.96249146e-01, 4.97620857e-01, 4.98846445e-01,\n",
       "       5.05682730e-01, 4.99833703e-01, 5.01337205e-01, 5.11688869e-01,\n",
       "       5.01181104e-01, 5.00706563e-01, 5.05943180e-01, 4.98494442e-01,\n",
       "       4.99477083e-01, 4.99197808e-01, 5.07077722e-01, 4.96121690e-01,\n",
       "       4.99803919e-01, 4.94642014e-01, 5.07063046e-01, 5.05733641e-01,\n",
       "       4.87000539e-01, 4.96012573e-01, 5.03031115e-01, 4.96634812e-01,\n",
       "       4.94676589e-01, 5.02832814e-01, 5.01690842e-01, 4.99790578e-01,\n",
       "       5.01566057e-01, 4.78701722e-01, 4.99203675e-01, 4.87212280e-01,\n",
       "       4.99604314e-01, 5.16324412e-01, 4.97132585e-01, 4.79643051e-01,\n",
       "       5.15039328e-01, 4.98237030e-01, 5.01844278e-01, 5.07457883e-01,\n",
       "       4.78719094e-01, 4.96278451e-01, 5.04148622e-01, 5.04698122e-01,\n",
       "       5.01128131e-01, 5.03546647e-01, 5.07358367e-01, 4.95434252e-01,\n",
       "       5.07945358e-01, 4.99646170e-01, 5.02559551e-01, 4.97408150e-01,\n",
       "       5.00446463e-01, 5.00990315e-01, 5.08923394e-01, 5.01528365e-01,\n",
       "       5.04220535e-01, 4.96217789e-01, 5.05706183e-01, 4.97926359e-01,\n",
       "       5.10106624e-01, 4.83630125e-01, 4.96938595e-01, 4.99795728e-01,\n",
       "       4.99924085e-01, 4.86367856e-01, 4.93552876e-01, 4.85425379e-01,\n",
       "       5.02972630e-01, 4.99167677e-01, 5.01947580e-01, 5.06825548e-01,\n",
       "       4.96850636e-01, 4.99641401e-01, 4.95613615e-01, 4.98291993e-01,\n",
       "       4.97769141e-01, 6.49254170e-02, 4.88472512e-01, 4.98598091e-01,\n",
       "       5.02291811e-01, 4.99148006e-01, 5.02275928e-01, 5.03962101e-01,\n",
       "       5.04249369e-01, 4.96384636e-01, 5.04016345e-01, 5.02472918e-01,\n",
       "       4.92666663e-01, 4.91499993e-01, 4.97809065e-01, 4.69291334e-01,\n",
       "       5.01739928e-01, 5.06587226e-01, 4.96065640e-01, 5.05560250e-01,\n",
       "       4.96955367e-01, 5.14955455e-01, 5.01785542e-01, 4.97697063e-01,\n",
       "       5.22339115e-01, 5.05936459e-01, 5.11480283e-01, 4.83722836e-01,\n",
       "       5.03191900e-01, 5.00091526e-01, 5.03063834e-01, 5.05482598e-01,\n",
       "       4.98246537e-01, 4.76762088e-01, 4.94126429e-01, 4.93989049e-01,\n",
       "       5.14433204e-01, 4.85678786e-01, 4.90600973e-01, 4.97152528e-01,\n",
       "       4.97868352e-01, 4.84733485e-01, 5.00884504e-01, 5.01208871e-01,\n",
       "       4.97138344e-01, 4.99292257e-01, 5.22804033e-01, 4.97052258e-01,\n",
       "       4.99043615e-01, 5.04500140e-01, 5.82918519e-01, 5.02143144e-01,\n",
       "       4.90792645e-01, 5.41830917e-01, 4.94884737e-01, 5.10497095e-01,\n",
       "       5.07288940e-01, 4.99258595e-01, 5.45993925e-01, 4.96606500e-01,\n",
       "       5.01417261e-01, 4.98472983e-01, 5.05593728e-01, 5.05172469e-01,\n",
       "       5.03175824e-01, 4.97947509e-01, 5.28356538e-01, 4.99372137e-01,\n",
       "       4.76145508e-01, 4.95065245e-01, 4.94884556e-01, 5.01463578e-01,\n",
       "       5.01850095e-01, 5.01308607e-01, 4.92533450e-01, 4.99424454e-01,\n",
       "       4.95654647e-01, 4.96337115e-01, 4.98517944e-01, 4.99604708e-01,\n",
       "       4.86419615e-01, 5.11445518e-01, 4.97753074e-01, 5.20019238e-01,\n",
       "       5.03255497e-01, 4.80613582e-01, 4.99817124e-01, 5.07544311e-01,\n",
       "       5.00343177e-01, 4.97994241e-01, 6.96405316e-01, 4.93867490e-01,\n",
       "       5.03662886e-01, 2.13164802e-01, 4.90448776e-01, 5.00438131e-01,\n",
       "       4.98306968e-01, 4.71573612e-01, 5.05267171e-01, 5.01115141e-01,\n",
       "       5.07723989e-01, 4.95922804e-01, 5.07522561e-01, 4.99508649e-01,\n",
       "       4.93645623e-01, 5.05652627e-01, 4.92447826e-01, 5.92337177e-01,\n",
       "       5.02500452e-01, 5.04362977e-01, 4.95883765e-01, 5.02171447e-01,\n",
       "       5.16556801e-01, 5.02299920e-01, 4.99488091e-01, 4.98649143e-01,\n",
       "       5.00301781e-01, 5.15772934e-01, 4.96880267e-01, 4.98092893e-01,\n",
       "       5.03649808e-01, 5.05738170e-01, 5.10516162e-01, 4.93957217e-01,\n",
       "       5.04375202e-01, 5.04421924e-01, 4.98281923e-01, 4.97110649e-01,\n",
       "       4.91330160e-01, 4.99301572e-01, 5.04317981e-01, 5.04707763e-01,\n",
       "       4.97729486e-01, 5.14433671e-01, 5.04369224e-01, 4.83212757e-01,\n",
       "       5.10332278e-01, 5.39201168e-01, 4.95672538e-01, 4.93247643e-01,\n",
       "       5.04554856e-01, 4.98588713e-01, 4.97733936e-01, 4.88998029e-01,\n",
       "       5.00136891e-01, 5.00394508e-01, 5.18654537e-01, 4.97550695e-01,\n",
       "       4.97476969e-01, 4.99672233e-01, 4.96419349e-01, 4.98789502e-01,\n",
       "       4.97072598e-01, 4.97129951e-01, 5.07462406e-01, 4.96221282e-01,\n",
       "       4.99060007e-01, 5.02691657e-01, 4.98576666e-01, 5.08990394e-01])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sig_(w1,b1,f_test):\n",
    "    z=w1*f_test+b1\n",
    "    s=1/(1+(np.exp(-z)))\n",
    "    return s\n",
    "sig_(w1,b1,f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
