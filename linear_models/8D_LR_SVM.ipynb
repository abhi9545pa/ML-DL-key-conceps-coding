{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86Tvnj5UblTy"
   },
   "source": [
    "## Task-D: Collinear features and their effect on linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qn_eOn2EblT3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMoYWIayblUB"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('task_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfStXG4tblUI",
    "outputId": "ddf4eec6-7f53-4d28-914f-23133957d6d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x*x</th>\n",
       "      <th>2*y</th>\n",
       "      <th>2*z+3*x*x</th>\n",
       "      <th>w</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.581066</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>-1.012978</td>\n",
       "      <td>-0.604025</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>-0.665927</td>\n",
       "      <td>-0.536277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.894309</td>\n",
       "      <td>-0.207835</td>\n",
       "      <td>-1.012978</td>\n",
       "      <td>-0.883052</td>\n",
       "      <td>-0.207835</td>\n",
       "      <td>-0.917054</td>\n",
       "      <td>-0.522364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.207552</td>\n",
       "      <td>0.212034</td>\n",
       "      <td>-1.082312</td>\n",
       "      <td>-1.150918</td>\n",
       "      <td>0.212034</td>\n",
       "      <td>-1.166507</td>\n",
       "      <td>0.205738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.364174</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>-0.943643</td>\n",
       "      <td>-1.280666</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>-1.266540</td>\n",
       "      <td>-0.665720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.737687</td>\n",
       "      <td>1.051772</td>\n",
       "      <td>-1.012978</td>\n",
       "      <td>-0.744934</td>\n",
       "      <td>1.051772</td>\n",
       "      <td>-0.792746</td>\n",
       "      <td>-0.735054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y         z       x*x       2*y  2*z+3*x*x         w  \\\n",
       "0 -0.581066  0.841837 -1.012978 -0.604025  0.841837  -0.665927 -0.536277   \n",
       "1 -0.894309 -0.207835 -1.012978 -0.883052 -0.207835  -0.917054 -0.522364   \n",
       "2 -1.207552  0.212034 -1.082312 -1.150918  0.212034  -1.166507  0.205738   \n",
       "3 -1.364174  0.002099 -0.943643 -1.280666  0.002099  -1.266540 -0.665720   \n",
       "4 -0.737687  1.051772 -1.012978 -0.744934  1.051772  -0.792746 -0.735054   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIIuomCkblUP"
   },
   "outputs": [],
   "source": [
    "X = data.drop(['target'], axis=1).values\n",
    "Y = data['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ydm98u3EblUU"
   },
   "source": [
    "### Doing perturbation test to check the presence of collinearity  \n",
    "\n",
    "#### Task: 1 Logistic Regression\n",
    "<pre>\n",
    "\n",
    "\n",
    "1. <b>Finding the Correlation between the features</b>\n",
    "    a. check the correlation between the features\n",
    "    b. plot heat map of correlation matrix using seaborn heatmap\n",
    "2. <b>Finding the best model for the given data</b>\n",
    "    a. Train Logistic regression on data(X,Y) that we have created in the above cell\n",
    "    b. Find the best hyper prameter alpha with hyper parameter tuning using k-fold cross validation (grid search CV or         \n",
    "    random search CV make sure you choose the alpha in log space)\n",
    "    c. Creat a new Logistic regression with the best alpha\n",
    "    (search for how to get the best hyper parameter value), name the best model as 'best_model'\n",
    "    \n",
    "3. <b>Getting the weights with the original data</b>\n",
    "    a. train the 'best_model' with X, Y\n",
    "    b. Check the accuracy of the model 'best_model_accuracy'\n",
    "    c. Get the weights W using best_model.coef_\n",
    "\n",
    "4. <b>Modifying original data</b>\n",
    "    a. Add a noise(order of 10^-2) to each element of X \n",
    "    and get the new data set X' (X' = X + e)\n",
    "    b. Train the same 'best_model' with data (X', Y)\n",
    "    c. Check the accuracy of the model 'best_model_accuracy_edited'\n",
    "    d. Get the weights W' using best_model.coef_\n",
    "    \n",
    "5. <b> Checking deviations in metric and weights </b>\n",
    "    a. find the difference between 'best_model_accuracy_edited' and 'best_model_accuracy'\n",
    "    b. find the absolute change between each value of W and W' ==> |(W-W')|\n",
    "    c. print the top 4 features which have higher % change in weights \n",
    "    compare to the other feature\n",
    "\n",
    "</pre>\n",
    "\n",
    "#### Task: 2 Linear SVM\n",
    "\n",
    "<pre>\n",
    "1. Do the same steps (2, 3, 4, 5) we have done in the above task 1.\n",
    "</pre>\n",
    "\n",
    "<strong><font color='red'>Do write the observations based on the results you get from the deviations of weights in both Logistic Regression and linear SVM</font></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1 Finding the Correlation between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lai8wXU1pmSb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAoKUlEQVR4nO3deZgU5bXH8e8Bhh2GRRiWQRgWESGKCOIC7iiSq4hR43LjgkgwbjdGIzGbMZq43JCrkciDSIxRY+KOBiQoooCKLKJsgsg6gGwadoSZPvePLsZm6GF6epqp7ub3yVMPXVVnqt4i0mfepd7X3B0REZFkVAu7ACIikrmUREREJGlKIiIikjQlERERSZqSiIiIJE1JREREkqYkIiKSBcxsrJltMLP5ZZw3M3vUzJaa2adm1iMV91USERHJDk8B/Q9y/nygU7ANBR5PxU2VREREsoC7vwd8dZCQgcDTHvUh0MjMWlb2vjUqe4Fssuvl3+n1fRFJSJ2L77bKXmPvpmUJf+fUbNbhh0RrEPuMdvfRFbhda2B1zH5hcGxdBa5xACUREZEMECSMiiSN0uIlvUr/4qwkIiISlkhxVd6tEGgTs58PrK3sRdUnIiISluKixLfKGwdcHYzSOgnY4u6VasoC1URERELjHknZtczs78AZwBFmVgj8GsiJ3sdHAeOBAcBSYCdwXSruqyQiIhKWSOqSiLtfUc55B25K2Q0DSiIiImFJYU0kLEoiIiJhqdqO9UNCSUREJCyqiYiISLI8NaOuQqUkIoe16YvX8NAbHxGJOIN6dWLwGd/Z7/xT781n/NxlABRHnOUbtvDOL75P7ZwaDB49gb1FEYoiEc7p1o4f9esewhNIRkthx3pYlETksFUcifD7cR8y6vpzyWtYl6tG/ovTu7ShQ16jkphrT+vGtad1A+DdRat5ZtpCcuvWwt15Ysh51K2Vw97iCNeNmkCfzq059shmIT2NZKQsaM7Sy4Zy2Jq/ehNtmjYkv0kDcmpU57zjCpiyaHWZ8RM+WU7/4woAMDPq1soBoKg4Whup9ERKcviJFCe+pamsTiJm1iuYN7+2mdUzswVm1i3sckl62LB1Jy1y65Xs5zWsy4YtO+LG7tpTxPtL1nBOt7Ylx4ojES57dBxn3f8PTurYiu+oFiIV5ZHEtzSV1UnE3WcSfdX/PuAh4Bl332/BFjMbamazzGzWk//+KIxiSkjizTxnFr8+8d5nq+netjm5dWuVHKterRr/vPVCJg6/lPmFm1j65deHqKSStap22pND4nDoE7kXmAnsBm4tfTJ2ZkxNBX94yWtYly9jah7rt+6kWcO6cWPfjGnKKq1hnZr0LMhj+pI1dGzR+JCUVbJUFnSsZ3VNJNAEqA80AGqHXBZJI13zj2DVpq2s+Wobe4uKmfjJck7vkn9A3Lbde5i9fD1nHvPtBKhfbd/N1l17ANi9t4gZX6yjoFlulZVdsoN7ccJbujocaiKjgV8CBcCDwM3hFkfSRY3q1Rh+YW9uHPsWEY8wsGcnOuY15oUZiwG4tHdnACYvWMXJnVpRp2ZOyc9u2raTX74wnYg7EXfO/U47TuvSJu59RMqUxn0dibLonFzZycyuBi5y94vNrDrwPvAzd58cL17NWSKSqFSsbLh7zriEv3Nq97gwLQcAZnVNxN2fBp4OPhcDvcMtkYhIjCyoiWR1EhERSWvFe8MuQaUpiYiIhCULRmcpiYiIhCULmrMOhyG+IiLpKRJJfCuHmfU3s8VmttTMhsc5n2tmr5vZJ8HsHVoeV0Qko6WoOSsYfToS6AcUAjPNbJy7L4wJuwlY6O4XmFkzYLGZPevueypzbyUREZGQeOo61k8Elrr7MgAzex4YCMQmEQcaWHRun/rAV0Cl51NRc5aISFgqMAFj7Dx/wTY05kqtgdgpqAuDY7EeA7oAa4F5wG3ule+UUU0kRoPLR4ZdBBHJEEV77q78RSrQnBU7z18c8V5ELP0i43nAXOAsoAMwycymuvvWhAsRh2oiIiJhSd1U8IVA7Lw7+URrHLGuA172qKXAcuDoyj6CkoiISFhSNzprJtDJzArMrCZwOdFlMGKtAs4GMLM8oDOwrLKPoOYsEZGwpOg9EXcvMrObgYlAdWCsuy8ws2HB+VHAb4GnzGwe0eavu9x9U2XvrSQiIhKWotQtNuXu44HxpY6Nivm8Fjg3ZTcMKImIiIQlC95YVxIREQmL5s4SEZGkqSYiIiJJU01ERESSlgU1Eb0nIoe9P464l88WTmPO7Ekc371bmXG/vfcuFi6YyrxPp3DzTYMBaNQolxdfGMOc2ZP4YPobdO3auaqKLdmgqCjxLU2pJiKHtfP7n0WnjgUcfUwfep/Yg5GP/Z5T+lxwQNw1V19Gfn4runY7DXenWbOmAPzsrlv45JMFXHLpEDp37sCfHvkd5/b/flU/hmQqT3iJ9bSlmogc1i644Dz+9uyLAMz4aA65jXJp0aL5AXHDfng1993/Rzz4R79x42YAunQ5ismTpwGwePEXtG2bT/PmR1RR6SXjpXA9kbBkdRIxs9+a2W0x+/eb2a1hlknSS+tWLShc/e0UQ2sK19G6VYsD4tq3b8dll17Ihx+M541xf6NjxwIAPp23kEEXDQCgV8/utG2bT37rllVTeMl8SiJp70ngGgAzq0Z0PplnYwNip1eORHaEUEQJU3Rphf15nCaGWrVqsnv3N5x08gDGjH2OMaP/AMCDDz1Go8a5zJr5b266aTAfz51PUXHxIS+3ZInUTcAYmqzuE3H3FWa22cyOB/KAj919c6mYkumVa9RsnfkNlFKuG4ddw/XXXwXArFlzyW/TquRc6/yWrF23/oCfKVyzjpdf+RcAr746gSefGAHAtm3bGXLD7SVxS5d8yPLlqw5l8SWbZMEvHNleEwEYA1xLdBrkseEWRdLB46P+Ss9e59Kz17mMGzeRH1x1CQC9T+zB1i1b+fLLDQf8zLhxb3LmGacCcPppJ7Pk8+jkp7m5DcnJyQHg+sFXMnXaDLZt215FTyIZLwuas7K6JhJ4BbgXyAGuDLkskmbGT3ib/v3PYvGi6ezctYshQ76tVbz+2tMMHXYn69at58GHRvK3vz7GbbfdwI7tO/nhsDsB6HJ0J/4y9hGKI8UsWrSEG4beEdajSCZK4+SQKIvX/pttzGwU8B93H36wODVniUiiivasibeaYIXsGnN7wt85dYaMqPT9DoWsr4kEHeonAZeGXRYRkVgeyfzfW7O6T8TMjgGWAm+7++dhl0dEZD/qE0lv7r4QaB92OURE4tLoLBERSVoKayJm1t/MFpvZUjOL2/9rZmeY2VwzW2Bm76biEbK6JiIiktZS1ExlZtWBkUA/oBCYaWbjgtaYfTGNgD8D/d19lZkdOL9PElQTEREJi3vi28GdCCx192Xuvgd4HhhYKuZK4GV3XxW9tR/4QlQSlERERMJSgeas2Cmagm1ozJVaA6tj9guDY7GOAhqb2RQzm21mV6fiEdScJSISlgoM8Y2doimOeO+QlL54DeAE4GygDvCBmX3o7ksSLkQcSiIxdq74d9hFEJHDSepGZxUCbWL284G1cWI2ufsOYIeZvQccB1Qqiag5S0QkJB6JJLyVYybQycwKzKwm0RnLx5WKeQ3oa2Y1zKwu0BtYVNlnUE1ERCQsKXpj3d2LzOxmYCJQHRjr7gvMbFhwfpS7LzKzN4FPgQgwxt3nV/beSiIiImFJ4Toh7j4eGF/q2KhS+w8DD6fspiiJiIiEJwvmzlISEREJS1HmT3uiJCIiEpY0XvY2UUoiIiJhUXOWiIgkK4Ghu2lPSUREJCyqiYhktmkfzeHBx8ZSXBzh4u+ew5ArL97v/JZt2/nVQ4+xeu16atXM4d6f3kSngrYsX7WGO+/9Q0lc4br13HTd5fzgkguq+hEkkymJiGSu4uJi7n/kCUY//GtaNGvK5cN+ypmn9KJDu29njxjz7Esc3bGAR347nGWrCvnd/z3BmBG/oeDI1rw4ZkTJdc6+9AbO7tM7rEeRTKVFqUQy17zPlnJkq5a0adWCnJwczj+rD+9M/2i/mC9WrKZ3j2MBaH9kPmvWb2DTV//ZL2bGnHm0aZVHqxYpWZ5BDiMe8YS3dJXVScTMhgWreM01s+Vm9k7YZZL0sWHTZlo0b1qyn9esKes3fbVfTOcO7XjrvQ8BmLfoc9Z9uZH1GzfvFzNh8jTOP7vvoS+wZJ+IJ76lqaxOIsF8Md2BXkRnsBxROiZ2jv4xz7xQ1UWUEMVb58dKTah9/ZUXs3X7di4ZcjvPvTKeozsVUKP6t/9s9u7dy5T3Z3Lu6acc4tJKVkrh8rhhOVz6RB4BJrv766VPxM7Rv2ftgvRN95Jyec2a8uWGb2sV6zdupnnTJvvF1K9Xl/vuugUAd6f/FcNo3TKv5PzUGR/T5aj2HNGkUZWUWbJMGtcwEpXVNREAM7sWaAv8JuSiSJrpdnRHVq5ZR+G69ezdu5cJk6dxxim99ovZun0He/fuBeClf73FCcceQ/16dUvOT5g8lfPP6lOl5ZYskgXNWVldEzGzE4A7gL7uWTC/gKRUjerVufvWIQz76b0URyIMOv9sOhYcyT/HTQTgsgvPY9nKQn7++0epVq0aHdrl85s7byr5+V27v+GD2Z/wq9uHhfUIkuG8OPO/lszLXwA+Y5nZX4DzgH0L0s9y9yFlxas5S0QSVbNV13hL0lbI1uv7Jfyd0/DJSZW+36GQ1TURd78u7DKIiJQlnYfuJirr+0RERNJWCvtEzKy/mS02s6VmNvwgcb3MrNjMLknFIyiJiIiEJVKB7SDMrDowEjgfOAa4wsyOKSPuQaLL6KZEVjdniYikMy9KWcf6icBSd18GYGbPAwOBhaXibgFeIvruXEqoJiIiEpYU1USA1sDqmP3C4FgJM2sNDAL2W3e9spRERERCUpG5s2Jn1wi2oTGXijdyq3RHyv8Bd7l7Smd9VHOWiEhYKtCaFTu7RhyFQJuY/XxgbamYnsDzFp3b5whggJkVufuriZfiQEoiIiIhSeEQ35lAJzMrANYAlwNX7ncv94J9n83sKeCNyiYQUBLZT/H7r4RdBBHJFJd0rfw1UtSv7u5FZnYz0VFX1YGx7r7AzIYF51PaDxJLSUREJCRelMJruY8Hxpc6Fjd5uPu1qbqvkoiISEiyYUY/JRERkbAoiYiISLJUExERkaQpiYiISNK8OC1nd68QJRERkZCoJiIiIknziGoiIiKSJNVERDLc9CVreOhfs4hEnEE9OzL49G77nX9q6gLGz10OQHEkwvKNW3nn7kvZtaeIX7w4nc3bd2FmfK9XJ646pUsYjyAZzF01kbRgZububmb3uPs9+/bDLpekt+JIhN+//hGjrjuHvIZ1uerxCZzeJZ8OzRuVxFzbtyvX9o1Ob/HuotU88/4icuvWYk9RMT85/wS6tG7Kjm/2csXIf3FSx5b7/axIebKhJpItU8H/2MyGAPXM7H6gX9gFkvQ3v3AzbZo0IL9JA3JqVOe8Y9syZdHqMuMnfLqC/sdG57Br1rAuXVo3BaBerRzaN8tlw9adVVJuyR6RYkt4S1cZl0SC9YE/NbPaZlbPzBYA/yY6tfGtwJvu/m8zG2Rmb1lUSzNbYmYtwi29pJMNW3fSIrdeyX5ew3ps2LIrbuyuPUW8//lazul65AHn1ny9nc/WfcV38o84ZGWV7OQRS3hLVxmXRNx9JjAOuA94CHgGOAfYBDwK9Dezfu7+CvAlcBPwBPBrd/+y9PViF3p5ctLMqnoMSQPxGjytjH+r731WSPcjm5Fbt9Z+x3d+s5c7nnuXO7/bi/q1ax6CUko2y4Ykkql9IvcSnT9/N9HaR6R0n0gQdwswH/jQ3f8e70KxC73sevE+9aMcRvJy6/Lllh0l++u37qBZwzpxY9/8dAX9jyvY79je4gg/ee5dBhxXwNlxaigi5cmGntuMq4kEmgD1gQZA7X2d6O5+T/Dnvv9rWhOd4izPzDL1WeUQ6dq6Kas2b2PNV9vYW1TMxE9XcvrRbQ6I27Z7D7NXrOfMLvklx9yd37z8AQXNc/lBn2OqstiSRVQTCc9o4JdAAfAgcHPpADOrAfyF6OpeVwO3A/9bhWWUNFejejWGX3AiNz71NhF3BvboSMe8RrwwYwkAl/Y+CoDJC1dzcseW1KmZU/Kzc1du5I25y+iU14jL/vQGALecezx9O7eu+geRjJUNQ3wt00bCmtnVwEXufrGZVQfeB37m7pNLxf0KaOTut5tZA6LNX4PcfVFZ11Zzlogkqs4lv6h0BljSpX/C3zlHLXozLTNOxtVE3P1p4OngczHQu4y4e2M+bwOOrpICiogkKJU1ETPrDzxCdHncMe7+QKnzVwF3BbvbgRvd/ZPK3jfjkoiISLZIVV9H0Cozkug7coXATDMb5+4LY8KWA6e7+9dmdj7RboG4v4RXhJKIiEhIUtibcCKw1N2XAZjZ88BAoCSJuPv7MfEfAvmkgEYsiYiEpCKjs2LfaQu2oTGXag3ETrdQGBwry/XAhFQ8g2oiIiIhKY4k/nt87DttccRrF4tbzzGzM4kmkT4J3/wglEREREKSwuasQiD2Jad8YG3pIDM7FhgDnO/um1NxYyUREZGQRFI3Omsm0MnMCoA1wOVE35ErYWZHAi8DP3D3Jam6sZKIiEhIUjXE192LzOxmYCLRIb5j3X2BmQ0Lzo8CfgU0Bf4czAxV5O49K3tvJRERkZCk8l1vdx8PjC91bFTM5yHAkNTdMSrj3lg/lGrUbK2/DBFJSNGeNZWuRszKvyjh75yeha/qjXUREflWRUZnpSslERGRkGRD04eSiIhISFI4Ois0SiIiIiHJhqnglUREREISCbsAKaAkIiISEo87W0lmURIREQlJkZqzREQkWaqJiIhI0rKhTyTz33QRqaQ/jriXzxZOY87sSRzfvVuZcb+99y4WLpjKvE+ncPNNgwFo1CiXF18Yw5zZk/hg+ht07dq5qootWcCxhLd0lXE1ETNrQ3SN9RZEE/lod38kOHctMAVY6ZrPRRJwfv+z6NSxgKOP6UPvE3sw8rHfc0qfCw6Iu+bqy8jPb0XXbqfh7jRr1hSAn911C598soBLLh1C584d+NMjv+Pc/t+v6seQDKWaSDiKgJ+4exfgJOAmMzvVzJ4EjiS60Mqog11AZJ8LLjiPvz37IgAzPppDbqNcWrRofkDcsB9ezX33/5F9v5ts3BhdiqFLl6OYPHkaAIsXf0Hbtvk0b35EFZVeMl0xlvCWrjIuibj7OnefE3zeBiwC6gJ3A4OJzqN/o5l1MLM5+37OzDqZ2ewwyizpq3WrFhSu/nbtnjWF62jdqsUBce3bt+OySy/kww/G88a4v9GxYwEAn85byKCLBgDQq2d32rbNJ791y6opvGS8iCW+pauMSyKxzKwdcDywGLgPGAv8Axjp7l8AW8ysexB+HfBUnGuUrFscieyoimJLGgnWVdhPvJbQWrVqsnv3N5x08gDGjH2OMaP/AMCDDz1Go8a5zJr5b266aTAfz51PUXHxIS+3ZIcIlvCWrjKuT2QfM6sPvAT8j7uvAm4I+kSmAs8EYWOA68zsduD7wImlrxO7brGmgj883DjsGq6//ioAZs2aS36bViXnWue3ZO269Qf8TOGadbz8yr8AePXVCTz5xAgAtm3bzpAbbi+JW7rkQ5YvX3Uoiy9ZJBu+cDKyJmJmOUQTyLPu/vK+4+7+lLuviOlUfwk4H/gvYHaq1hSWzPb4qL/Ss9e59Ox1LuPGTeQHV10CQO8Te7B1y1a+/HLDAT8zbtybnHnGqQCcftrJLPl8GQC5uQ3JyckB4PrBVzJ12gy2bdteRU8imS5SgS1dZVwSsWj7w5PAIncfcbBYd99NdLnIx4G/VEHxJMOMn/A2y5avYvGi6Ywa9RA333J3ybnXX3uali3zAHjwoZFcPOi7fDznLe6/72f8cNidAHQ5uhOfzn2H+fPepX//M/nx7b8K5TkkM0XMEt7KY2b9zWyxmS01s+FxzpuZPRqc/9TMeqTiGTJuZUMz60O0yWoe3ybou4OlIePFn0S0RnKkux+0sVrNWSKSqFSsbPiPllcl/J3z/XXPlnk/M6sOLAH6AYXATOAKd18YEzMAuAUYAPQGHnH33kkWvUTG9Ym4+zSoUC9TH6KL1qu3U0TSSgpHXZ0ILHX3ZQBm9jwwEFgYEzMQeDpo7v/QzBqZWUt3X1eZG2dcEqkIM3sF6ACcFXZZRERKq8ioKzMbCgyNOTQ6GBgE0BpYHXOukGhtI1a8mNaAkkhZ3H1Q2GUQESlLRdrPY0eSxhEvG5W+fCIxFZbVSUREJJ2lsDmrEGgTs58PrE0ipsIybnSWiEi2SOEQ35lAJzMrMLOaRGfuGFcqZhxwdTBK6yRgS2X7Q0A1ERGR0BSnqCbi7kVmdjPRVxqqEx1MtMDMhgXnRwHjiY7MWgrsJDqLR6UpiYiIhCSVLxEGrzmML3VsVMxnB25K4S0BJRERkdCk85voiVISibHttbvCLoKIHEayYIl1JRERkbCoJiIiIknLhmk0lEREREKSzotNJUpJREQkJGrOEhGRpCmJiIhI0rJh7QklERGRkKhPREREkqbRWSIikrRIFjRoKYnIYW36opU89PI0Ih5h0EnHMPicE/Y7/9TkOYyftQSA4oizfP3XvHPfYHLr1Wbrzm+49x/vsHTdZgzjnivO4riCFmE8hmSow6Jj3czaAE8DLYg+82h3fyQ4dy0wBVjpSS7WbmZtgZeJzjyZA/zJ3UeZmbm7m9k97n7Pvv0yrpFwrMg+xZEIv3/xPUbdeCF5jepz1YgXOL1bAR1aNCmJufasHlx7Vg8A3p2/nGfe/YTcerUBeOiVqZxy9JH873X92VtUzK49RaE8h2SubPiSSmQ9kSLgJ+7eBTgJuMnMTjWzJ4Ejia5hPupgF9jHzKaYWbtSh9cBp7h7d6LLOQ43s1ZAdzN7FGhiZhcB9x/k0uea2f1APTMbAvxPIuWRw9v8lRtoc0Qu+UfkklOjOucd34kp85aXGT9hzuf079EJgO279zDni7UMOqkLADk1qtOwbq0qKbdkjxSuJxKacmsiwaIl64LP28xsEVAXuBuYAcwHLgy++GOnIf4O0N7dV5Zz/T0xu7UIEpu7f2xmu4APgBx3v9HMcoGPgAvdfbGZ/R2Y7O5PBLGTgF+5+4NBDect4GTgK+Bd4Lfu/u/ynlkODxu2bKdF4/ol+3mN6jNv5fq4sbv27OX9z1bxs++dBkDhpi00rl+HXz03mSVrN3FMm2b8dFBf6tTKqZKyS3Yossyvi1RoZcOgFnE8sBi4DxgL/AMY6e5r3b17UKN4AnipvAQSc902ZvYp0UXkH3T3tWbWHfgR8Aww0czuc/ctwM3AU2Z2OdA4SCD9gPOAR4HNZnZbcO8HidaSfgIsjJdAzGyomc0ys1lPTni/In8dkuHi/fO1MoZcvjd/Bd0LWpY0ZRVHnM8KN3LZqV35x53fp3bNHMa+PefQFVaykldgS1cJJxEzqw+8BPyPu69y9xuAVcBUol/2++JOBYYAg4P968xsrpnNBXoC44P9V/b9jLuvdvdjgY7ANWaWB3zi7rcCm939VeCXQewkYB4wMrgPwFvu/nNgh7uPIZpMCD43AIYBd8R7Lncf7e493b3n9eefkuhfh2SBvNz6fPn19pL99f/ZTrOG9eLGvvnxt01ZAHmN6tE8tz7faRftSO93XAcWFW48tAWWrJMNzVkJJREzyyGaQJ5195f3HXf3p9x9xb5ObDNrCTwJfN/dtwcxf4mpocwCBgT7g0rfx93XAguAvvuu6e73BH/uu0c1oAuwC2gSey5ObF2ii9EDfNtuIQJ0PbI5qzZtYc3mrewtKmbix59zerd2B8Rt2/UNs79Yy5ndCkqOHdGwHi0a12fF+q8BmLGkkPZ5jauq6JIlInjCW2WYWRMzm2Rmnwd/HvAfa9Ai9I6ZLTKzBWZ2WyLXTmR0lhFNDIvcfcRB4nKAfwJ3ufuSRG4e/Fw+0drGruDBTgXKvA/wY2AR0T6ZsWZ2srvvLSP2QeBZYCXRJrb/SrRckv1qVK/G8O/15cZR44hEnIG9u9CxZVNemD4fgEtP7QbA5E+XcXLnNgf0d9x1cV/ufmYSe4sitG7akHuvPKvKn0EyWxU2Uw0H3nb3B8xseLBfehW+fYOo5phZA2C2mU1y94UHu7CVNxLWzPoQbbKax7e1qruD9Xxj404nukj8ZzGHBwS1i30xU4Br3X1FzLF+wB+I/n0a8Ji7jy6jLEcBrwEnBp38I4Bt7v7rOLGnE00ip7p7sZm9DLzu7n8p61l3TXg0nZseRSSN1Dn/1kpPWnJHuysS/s753xV/T/p+ZrYYOMPd1wUtRlPcvXM5P/Ma0e/jSQeLS2R01jSiX+7lxb0L1C4n5ow4xyYBx5Z3/SB2CdGmrH37t5dTnpNi9i9O5B4iIlWluAJ1ETMbCgyNOTS6rF+448gLRtoSJJLm5dyrHdFBVDPKu7DeWBcRCUlFOsyDhFFm0jCzt4i+FF7azytSplKDqLaWF68kIiISEk9hr4i7n1PWOTNbb2YtY5qzNpQRF3cQ1cFU6D0RERFJnSoc4jsOuCb4fA3RvuX9JDqIqjQlERGRkFTVEF/gAaCfmX0O9Av2MbNWZrZvkNSpwA+As/a922dmA8q7sJqzRERCUlXDQd19M3B2nONrgQHB54QGUZWmJCIiEpKitJ7QJDFKIiIiIUllx3pYlERi+Movwi6CiBxG0nlOrEQpiYiIhEQ1ERERSZpqIiIikrTiLFjFW0lERCQkKXj/I3RKIiIiIVGfiIiIJE19IiIikjQ1Z4mISNLUnCUiIknT6CyRDDd9xUYenrKISAQu6pbP4BPbHxAza/VmHn73M4qKnUZ1cnjyst4l54ojzlXPvU/z+rV59KITqrLokgXUnCWSwYojzgOTF/L4xb3Ia1Cbq577gNM7NKdD0/olMdt27+V3kxcyclBPWjasw1c7v9nvGs99vIKCJvXZsaeoqosvWSAbOta1nogctuZ/+R/aNKpLfqO65FSvxnmdWzDli/X7xUxYvI6zO+bRsmEdAJrUrVVybv223UxbvpFB3fKrtNySPbwC/0tXWZdEzOynZnZr8PmPZjY5+Hy2mT0TbukknWzY/g15DeqU7OfVr83G7fvXNFZ+vYOt3xQx5IUZXPns+7y+cE3JuYenLOK2vp2pVuEVGESiqnBRqkMm65II8B7QN/jcE6gfrBvcB5haOtjMhprZLDObNXbq/CospqSlUgmhOOIsWr+FP110AiMv7skTM75g5dc7eG/ZBprUrckxebnhlFOygrsnvFWGmTUxs0lm9nnwZ+ODxFY3s4/N7I1Erp2NfSKzgRPMrAHwDTCHaDLpC9xaOtjdRwOjAXaOui19072kXPP6tVi/bVfJ/vrtu2lWr1apmNo0qpNDnZwa1MmBHq0bs2TjNhZt2MK7yzYwbcVG9hRF2LGniJ9P+IT7zz+uqh9DMlhx1dUwhgNvu/sDZjY82L+rjNjbgEVAw0QunHU1EXffC6wArgPeJ1r7OBPoQPQvRgSAri1yWfX1TtZs2cne4ggTF3/JGe2b7xdzRofmfLzma4oiEXbtLWb+l1soaFKPW/t0ZuINZzL++jN4YMBx9GrTVAlEKqwKm7MGAn8NPv8VuChekJnlA98FxiR64WysiUC0SesOYDAwDxgBzPbK1gklq9SoVo27zjqGH708i4g7A7vm0+GIBrzwySoALj3uSNo3rc8p7Zpx2d+mU82MQd3y6XhEg5BLLtmiIl9JZjYUGBpzaHTQkpKIPHdfF9xznZk1LyPu/4CfAgn/R56tSWQq8HPgA3ffYWa7idMfItK3oBl9C5rtd+zS447cb/+angVc07OgzGv0bNOUnm2aHpLySXarSA0jtuk9HjN7C2gR59TPE7m+mf0XsMHdZ5vZGYmWKyuTiLu/DeTE7B8VYnFEROJK5dBddz+nrHNmtt7MWga1kJbAhjhhpwIXmtkAoDbQ0Myecff/Pth9s65PREQkUxS7J7xV0jjgmuDzNcBrpQPc/Wfunu/u7YDLgcnlJRBQEhERCU0Vdqw/APQzs8+BfsE+ZtbKzMZX5sJZ2ZwlIpIJquolQnffDJwd5/haYECc41OAKYlcW0lERCQk2TBgVElERCQk6TydSaKUREREQpLOEysmSklERCQkxZ75k8EricTIueS2sIsgIocR9YmIiEjS1CciIiJJU5+IiIgkLaLmLBERSZZqIiIikjSNzhIRkaSpOUtERJKm5iwREUmaaiIiIpI01UREstgvfjeC96Z/RJPGjXj1mVFhF0eyULEXh12ESgttUSoza2RmP6qC+1xkZscc6vtI9rloQD9Gjbgv7GJIFnP3hLd0FebKho2AhJOIRSVT3osAJRGpsJ7dv0NuwwZhF0OyWBWubHjIhJlEHgA6mNlcM/ujmb1tZnPMbJ6ZDQQws3ZmtsjM/gzMAdqY2S/N7DMzm2RmfzezO4LYDmb2ppnNNrOpZna0mZ0CXAg8HNynQ2hPKyJSSlXVRMysSfCd+XnwZ+My4hqZ2YvBd+wiMzu5vGuHmUSGA1+4e3fgTmCQu/cAzgT+YGYWxHUGnnb344FmwPeA44GLgZ4x1xsN3OLuJwB3AH929/eJLlB/p7t3d/cvShfCzIaa2SwzmzXm6b8fkgcVEYkn4p7wVknDgbfdvRPwdrAfzyPAm+5+NHAcsKi8C6dLx7oBvzOz04AI0BrIC86tdPcPg899gNfcfReAmb0e/FkfOAV44dvcQ61Ebuzuo4kmIPZuWpa+dUYRyTpVODprIHBG8PmvRNdPvys2wMwaAqcB1wK4+x5gT3kXTpckchXRWsYJ7r7XzFYAtYNzO2LirPQPBqoB/wlqNSIiGaEi056Y2VBgaMyh0cEvwYnIc/d1AO6+zsyax4lpD2wE/mJmxwGzgdvcfUec2BJhNmdtA/b1WuYCG4IEcibQtoyfmQZcYGa1g9rHdwHcfSuw3MwuhZJO+OPi3EckYXf++gGu+uGPWbGqkLMv+m9een1i2EWSLFORPhF3H+3uPWO2/RKImb1lZvPjbAMTLE4NoAfweNB9sIOym732+6FQuPtmM5tuZvOBmcDRZjYLmAt8VsbPzDSzccAnwEpgFrAlOH0V8LiZ/QLIAZ4P4p4HnjCzW4FL4vWLiMTz8G/K/fcjUimpfGPd3c8p65yZrTezlkEtpCWwIU5YIVDo7jOC/RdJIIlYOo8/jsfM6rv7djOrC7wHDHX3Oam4tvpERCRROUe0L6t5PWGN63dM+Dvn6+1Lk76fmT0MbHb3B8xsONDE3X8aJ24qMMTdF5vZPUA9d7/zYNdOlz6RihgdvDxYG/hrqhKIiEhVq8L3Px4A/mlm1wOrgH1N/62AMe4+IIi7BXjWzGoCy4DryrtwxtVEDiXVREQkUamoiTSs1z7h75ytO5ZV+n6HQibWREREsoIWpRIRkaRpKngREUlaNnQnKImIiIRE64mIiEjSVBMREZGkZUOfiIb4ipTDzIZWYI4ikcNKmHNniWSKoeWHiByelERERCRpSiIiIpI0JRGR8qk/RKQM6lgXEZGkqSYiIiJJUxIREZGkKYmIHISZ9TezxWa2NFjMR0RiqE9EpAxmVh1YAvQjunToTOAKd18YasFE0ohqIiJlOxFY6u7L3H0P8DwwMOQyiaQVJRGRsrUGVsfsFwbHRCSgJCJStnjLkar9VySGkohI2QqBNjH7+cDakMoikpaURETKNhPoZGYFZlYTuBwYF3KZRNKK1hMRKYO7F5nZzcBEoDow1t0XhFwskbSiIb4iIpI0NWeJiEjSlERERCRpSiIiIpI0JREREUmakoiIiCRNSURERJKmJCIiIkn7f9tNMlIiG0WTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "corr_df=data.corr()['target']\n",
    "corr=np.array(corr_df).reshape(8,1)\n",
    "ylabel=list(data)                                       #ylabel from data heading\n",
    "sn.heatmap(data=corr,annot=True,yticklabels=ylabel)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2 Finding the best model for the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter =  {'C': 1e-05}\n",
      "best score =  1.0\n",
      "best estimator =  LogisticRegression(C=1e-05)\n"
     ]
    }
   ],
   "source": [
    "# step 2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "parameters = {'C':(np.logspace(-5,2))}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n",
    "#https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/\n",
    "\n",
    "log_reg=LogisticRegression()\n",
    "log_reg_cv = GridSearchCV(log_reg, parameters,n_jobs=-1,return_train_score=True,cv=cv)\n",
    "log_reg_cv.fit(X, Y)                                     # fitting on whole data for find out best hyperparameter \n",
    "\n",
    "\n",
    "\n",
    "print(\"best parameter = \",log_reg_cv.best_params_)\n",
    "print(\"best score = \",log_reg_cv.best_score_)\n",
    "print(\"best estimator = \",log_reg_cv.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3 Getting the weights with the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef :- [[ 0.00032072 -0.00028317  0.00039242  0.00031842 -0.00028317  0.00033396\n",
      "   0.00024287]]\n",
      "accuracy =  1.0\n"
     ]
    }
   ],
   "source": [
    "# step 3\n",
    "best_model=LogisticRegression(C=1e-05)\n",
    "best_model.fit(X_train,y_train)             #fitting training data on best_hyperparameter_model \n",
    "y_pred=best_model.predict(X_test)\n",
    "best_model_accuracy=accuracy_score(y_test, y_pred)  #getting the accuracy\n",
    "\n",
    "weight=best_model.coef_\n",
    "print(\"coef :-\",weight)\n",
    "print(\"accuracy = \",best_model_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4 Modifying original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.54982604  0.8782776  -0.96722296 -0.55973594  0.88198457 -0.62323514\n",
      " -0.47674957] [-0.5810659   0.84183714 -1.01297765 -0.60402468  0.84183714 -0.66592679\n",
      " -0.53627703]\n",
      "[-0.81266249 -0.17488108 -0.93877223 -0.86847255 -0.18493569 -0.90517623\n",
      " -0.51190884] [-0.89430898 -0.2078351  -1.01297765 -0.88305213 -0.2078351  -0.91705408\n",
      " -0.52236404]\n",
      "[-1.14746935  0.26406456 -1.07219399 -1.11386733  0.25029099 -1.10460022\n",
      "  0.28240544] [-1.20755205  0.21203379 -1.08231219 -1.15091848  0.21203379 -1.16650718\n",
      "  0.20573767]\n",
      "[-1.29133121  0.04938639 -0.89555657 -1.26390788  0.03886202 -1.19533643\n",
      " -0.63090284] [-1.36417359  0.00209934 -0.94364311 -1.28066624  0.00209934 -1.26653955\n",
      " -0.66571996]\n",
      "[-0.71112868  1.07804986 -0.9917153  -0.73310346  1.12312801 -0.75994972\n",
      " -0.65879822] [-0.73768744  1.05177159 -1.01297765 -0.74493354  1.05177159 -0.79274607\n",
      " -0.73505412]\n",
      "[-0.07900231  1.75295755 -0.7180315  -0.11246259  1.75903396 -0.20607379\n",
      "  0.57502377] [-0.11120129  1.68157493 -0.80497402 -0.16455644  1.68157493 -0.24528858\n",
      "  0.48991585]\n",
      "[-1.31561018  0.70023581 -0.95721813 -1.20080635  0.71222121 -1.25152081\n",
      "  0.41409749] [-1.36417359  0.63190269 -1.01297765 -1.28066624  0.63190269 -1.27491046\n",
      "  0.3681559 ]\n",
      "[-0.72483959  0.66054586 -0.91441807 -0.70426534  0.70492334 -0.69898973\n",
      " -1.84598743] [-0.73768744  0.63190269 -0.94364311 -0.74493354  0.63190269 -0.78437516\n",
      " -1.8752551 ]\n",
      "[-1.63184112 -0.3279229  -0.92593123 -1.52145457 -0.35464875 -1.47931727\n",
      "  0.20704184] [-1.67741667 -0.41776955 -1.01297765 -1.53179095 -0.41776955 -1.50092502\n",
      "  0.16383818]\n",
      "[-0.83222224  0.03698596 -0.91105012 -0.86916326  0.01600491 -0.83933239\n",
      "  0.27696885] [-0.89430898  0.00209934 -0.94364311 -0.88305213  0.00209934 -0.90868317\n",
      "  0.25601337]\n",
      "[-0.04693667  1.28079156 -0.91111899 -0.1152204   1.30575853 -0.20097135\n",
      " -0.29174106] [-0.11120129  1.26170604 -0.94364311 -0.16455644  1.26170604 -0.2620304\n",
      " -0.36363171]\n",
      "[-1.00914309  0.70783163 -0.85953528 -0.99395273  0.67991672 -0.93697625\n",
      " -1.62765169] [-1.05093052  0.63190269 -0.87430856 -1.01838044  0.63190269 -1.02210899\n",
      " -1.67303624]\n",
      "[-0.99895109 -0.17073609 -0.93771401 -0.96198394 -0.14914574 -0.98065932\n",
      " -1.58528621] [-1.05093052 -0.2078351  -1.01297765 -1.01838044 -0.2078351  -1.03885081\n",
      " -1.66480188]\n",
      "[-1.75902488 -0.14172683 -1.15101956 -1.61993357 -0.13654671 -1.57823378\n",
      " -1.31874748] [-1.8340382  -0.2078351  -1.22098127 -1.65316789 -0.2078351  -1.63527812\n",
      " -1.37191487]\n",
      "[ 0.54799112  1.96941297 -1.08870341  0.50749251  1.95061542  0.28733211\n",
      " -0.76471827] [ 0.51528486  1.89150938 -1.15164673  0.46046505  1.89150938  0.275382\n",
      " -0.83675616]\n",
      "[ 0.38649419  2.74386028 -0.91221843  0.33238267  2.77318203  0.21894426\n",
      " -0.0896765 ] [ 0.35866332  2.73124718 -0.94364311  0.30002427  2.73124718  0.15609654\n",
      " -0.14465006]\n",
      "[-0.08982806  1.75363929 -1.03897158 -0.07682409  1.75028497 -0.21695036\n",
      " -0.1959385 ] [-0.11120129  1.68157493 -1.08231219 -0.16455644  1.68157493 -0.27877222\n",
      " -0.25971271]\n",
      "[-0.50428391  0.90693879 -0.96126849 -0.52795395  0.8889886  -0.64535845\n",
      " -0.36058735] [-0.5810659   0.84183714 -1.01297765 -0.60402468  0.84183714 -0.66592679\n",
      " -0.40651976]\n",
      "[ 0.39966425  1.53287117 -0.74850879  0.3308944   1.52347348  0.22191053\n",
      "  0.05265426] [ 0.35866332  1.47164049 -0.80497402  0.30002427  1.47164049  0.17283836\n",
      "  0.00270811]\n",
      "[-0.5507324   1.49980427 -0.92987871 -0.52707686  1.55602508 -0.58004266\n",
      "  0.45524094] [-0.5810659   1.47164049 -0.94364311 -0.60402468  1.47164049 -0.65755588\n",
      "  0.3952297 ]\n",
      "[-0.0866168   0.70628776 -0.72850027 -0.07774121  0.70987808 -0.15910186\n",
      " -1.58903552] [-0.11120129  0.63190269 -0.80497402 -0.16455644  0.63190269 -0.24528858\n",
      " -1.61253425]\n",
      "[-0.52955971  1.3288752  -0.89753296 -0.58710133  1.29775247 -0.57057556\n",
      " -1.1366856 ] [-0.5810659   1.26170604 -0.94364311 -0.60402468  1.26170604 -0.65755588\n",
      " -1.18216914]\n",
      "[-1.341605    1.06278603 -1.22170504 -1.23876211  1.13755725 -1.28245895\n",
      " -1.47845679] [-1.36417359  1.05177159 -1.29031581 -1.28066624  1.05177159 -1.3083941\n",
      " -1.51203058]\n",
      "[-0.55533992  0.46971452 -0.75516682 -0.56597572  0.48615704 -0.60220234\n",
      " -0.07780095] [-0.5810659   0.42196824 -0.80497402 -0.60402468  0.42196824 -0.64081406\n",
      " -0.09799583]\n",
      "[-1.00405024  0.71776471 -0.63946845 -0.96582991  0.68431887 -0.95759682\n",
      "  0.35936184] [-1.05093052  0.63190269 -0.66630494 -1.01838044  0.63190269 -0.99699626\n",
      "  0.34077829]\n",
      "[-0.69755497 -0.18254041 -0.82130038 -0.69148378 -0.16444654 -0.73139337\n",
      " -0.71307767] [-0.73768744 -0.2078351  -0.87430856 -0.74493354 -0.2078351  -0.77600425\n",
      " -0.779964  ]\n",
      "[-0.70203535  0.65358572 -0.81530313 -0.71571349  0.65381733 -0.74352219\n",
      " -1.02155532] [-0.73768744  0.63190269 -0.87430856 -0.74493354  0.63190269 -0.77600425\n",
      " -1.04470299]\n",
      "[-0.39981704  0.89426294 -0.91707816 -0.40284505  0.87209479 -0.49166312\n",
      " -0.39068081] [-0.42444437  0.84183714 -0.94364311 -0.46032554  0.84183714 -0.52822532\n",
      " -0.44449216]\n",
      "[-0.39795235  0.69920905 -0.97765229 -0.43455623  0.71654513 -0.50379587\n",
      " -1.01505017] [-0.42444437  0.63190269 -1.01297765 -0.46032554  0.63190269 -0.53659623\n",
      " -1.10420653]\n",
      "[-1.11957403  0.22426865 -0.7956473  -1.11089998  0.29673602 -1.06670875\n",
      " -1.89362162] [-1.20755205  0.21203379 -0.87430856 -1.15091848  0.21203379 -1.14139445\n",
      " -1.9312688 ]\n",
      "[-1.0016356   0.05998495 -0.79268912 -0.99244438  0.0640793  -0.94934561\n",
      " -0.93114551] [-1.05093052  0.00209934 -0.87430856 -1.01838044  0.00209934 -1.02210899\n",
      " -0.98334579]\n",
      "[-0.04217907  0.72019231 -0.88108086 -0.10377241  0.71348864 -0.18936488\n",
      "  0.55778719] [-0.11120129  0.63190269 -0.94364311 -0.16455644  0.63190269 -0.2620304\n",
      "  0.5033349 ]\n",
      "[-0.34228987  2.18387711 -0.88378392 -0.4230712   2.17108947 -0.45981147\n",
      "  0.36021759] [-0.42444437  2.10144383 -0.94364311 -0.46032554  2.10144383 -0.52822532\n",
      "  0.27889566]\n",
      "[ 0.08857758  2.35739012 -0.93182879  0.01711299  2.34382282 -0.10543983\n",
      " -0.60916776] [ 0.04542025  2.31137828 -1.01297765 -0.01248648  2.31137828 -0.13353694\n",
      " -0.68404641]\n",
      "[-0.81802535  0.0276403  -0.8594001  -0.82080463  0.06467113 -0.83729418\n",
      " -0.07510013] [-0.89430898  0.00209934 -0.94364311 -0.88305213  0.00209934 -0.90868317\n",
      " -0.11431202]\n",
      "[-0.65308664  0.2787501  -1.11236381 -0.72228463  0.29542033 -0.78218829\n",
      " -1.02158564] [-0.73768744  0.21203379 -1.15164673 -0.74493354  0.21203379 -0.80948789\n",
      " -1.03823234]\n",
      "[ 0.0757035   0.85580943 -1.03092417  0.01261525  0.90213974 -0.07486296\n",
      " -0.17136274] [ 0.04542025  0.84183714 -1.08231219 -0.01248648  0.84183714 -0.14190785\n",
      " -0.24485643]\n",
      "[-0.86309711  1.09637486 -1.00223405 -0.81189954  1.1270769  -0.83280183\n",
      " -1.01530501] [-0.89430898  1.05177159 -1.01297765 -0.88305213  1.05177159 -0.91705408\n",
      " -1.0870547 ]\n",
      "[-1.63206218 -0.16493171 -1.01893621 -1.50697876 -0.19281096 -1.46499624\n",
      " -0.00554167] [-1.67741667 -0.2078351  -1.08231219 -1.53179095 -0.2078351  -1.50929593\n",
      " -0.04728291]\n",
      "[-0.5244678   0.7071361  -0.86153865 -0.54917318  0.64739133 -0.60871612\n",
      " -1.8495642 ] [-0.5810659   0.63190269 -0.94364311 -0.60402468  0.63190269 -0.65755588\n",
      " -1.89048402]\n",
      "[-0.6718604   0.9295648  -1.05661515 -0.7199183   0.91743299 -0.74048154\n",
      " -1.1299333 ] [-0.73768744  0.84183714 -1.08231219 -0.74493354  0.84183714 -0.80111698\n",
      " -1.20189106]\n",
      "[-1.4688837  -1.64785561 -1.04138882 -1.36121879 -1.65549125 -1.32737239\n",
      "  0.47380041] [-1.52079513 -1.67737625 -1.08231219 -1.40762373 -1.67737625 -1.39754429\n",
      "  0.43992693]\n",
      "[-1.62616019  0.24168018 -1.03602716 -1.48455954  0.26966652 -1.44247114\n",
      " -1.90579468] [-1.67741667  0.21203379 -1.08231219 -1.53179095  0.21203379 -1.50929593\n",
      " -1.93008723]\n",
      "[-0.68547104  0.88949405 -0.83056048 -0.72325213  0.9190289  -0.73220317\n",
      " -0.41416292] [-0.73768744  0.84183714 -0.87430856 -0.74493354  0.84183714 -0.77600425\n",
      " -0.49462875]\n",
      "[-0.51656841  1.54845951 -0.63682003 -0.53679609  1.52250288 -0.5965533\n",
      " -1.35577391] [-0.5810659   1.47164049 -0.66630494 -0.60402468  1.47164049 -0.62407224\n",
      " -1.3930365 ]\n",
      "[-1.01359088 -0.18759221 -0.94424677 -0.93650944 -0.12963806 -0.99182635\n",
      " -1.36786757] [-1.05093052 -0.2078351  -1.01297765 -1.01838044 -0.2078351  -1.03885081\n",
      " -1.43304574]\n",
      "[-0.51099243  1.50739893 -0.84098714 -0.58532097  1.53536701 -0.58919243\n",
      " -1.01466093] [-0.5810659   1.47164049 -0.87430856 -0.60402468  1.47164049 -0.64918497\n",
      " -1.06009817]\n",
      "[-1.2962062   0.22554636 -0.94489562 -1.27010445  0.26971858 -1.26061329\n",
      "  0.15063025] [-1.36417359  0.21203379 -1.01297765 -1.28066624  0.21203379 -1.27491046\n",
      "  0.07875195]\n",
      "[-0.20488302  1.34784031 -0.90752591 -0.23399596  1.30578237 -0.36946646\n",
      " -1.32363114] [-0.26782283  1.26170604 -0.94364311 -0.31383613  1.26170604 -0.3963835\n",
      " -1.35689221]\n",
      "[-0.69640662  0.50718027 -0.94415248 -0.66452215  0.44386636 -0.72610146\n",
      "  0.14564425] [-0.73768744  0.42196824 -1.01297765 -0.74493354  0.42196824 -0.79274607\n",
      "  0.11354893]\n",
      "[2.45132747 0.25570563 1.34434525 2.67574776 0.22364969 2.56197366\n",
      " 1.61407825] [2.39474331 0.21203379 1.27506221 2.60339588 0.21203379 2.49702141\n",
      " 1.57480018]\n",
      "[1.47362281 0.28810177 1.21556752 1.50758533 0.22955876 1.48260568\n",
      " 1.1679626 ] [1.45501408 0.21203379 1.13639313 1.48170552 0.21203379 1.47074789\n",
      " 1.09369365]\n",
      "[2.26629469 0.01290158 1.49128939 2.48758538 0.07750735 2.39156961\n",
      " 1.72362754] [2.23812177e+00 2.09934449e-03 1.41373130e+00 2.40947180e+00\n",
      " 2.09934449e-03 2.33922977e+00 1.67479348e+00]\n",
      "[ 1.24239254e-01 -1.64564707e+00  8.05024184e-01 -2.22858700e-04\n",
      " -1.64846781e+00  1.25993644e-01  1.10832595e+00] [ 0.04542025 -1.67737625  0.78972042 -0.01248648 -1.67737625  0.08410671\n",
      "  1.08052043]\n",
      "[ 1.65279755 -0.5780556   1.24678872  1.6822498  -0.54855345  1.70031559\n",
      " -0.67124118] [ 1.61163562 -0.627704    1.20572767  1.66167823 -0.627704    1.6410959\n",
      " -0.70031188]\n",
      "[ 0.41370055 -0.56577836  1.18365767  0.37505107 -0.58899207  0.4827114\n",
      " -0.02707794] [ 0.35866332 -0.627704    1.13639313  0.30002427 -0.627704    0.40722383\n",
      " -0.0505433 ]\n",
      "[1.32775717 0.49596578 1.30076746 1.33898609 0.47620946 1.35035204\n",
      " 1.26126287] [1.29839254 0.42196824 1.27506221 1.30452309 0.42196824 1.32802388\n",
      " 1.24065846]\n",
      "[-0.82429774 -1.42806074  0.32712836 -0.870835   -1.43596647 -0.6953916\n",
      "  1.04697035] [-0.89430898 -1.4674418   0.30437864 -0.88305213 -1.4674418  -0.75800679\n",
      "  1.01070437]\n",
      "[ 1.78161125 -0.34665931  1.29310907  1.89737416 -0.35980595  1.83926253\n",
      "  1.16745638] [ 1.76825716 -0.41776955  1.20572767  1.84444121 -0.41776955  1.80558428\n",
      "  1.08190246]\n",
      "[-0.36534653 -0.82316393  0.76861977 -0.43324792 -0.8113451  -0.28236523\n",
      " -0.6389103 ] [-0.42444437 -0.83763845  0.72038588 -0.46032554 -0.83763845 -0.32732349\n",
      " -0.65530013]\n",
      "[-0.67401424 -2.22985542  0.47539714 -0.69189194 -2.25193898 -0.58071636\n",
      " -0.40875281] [-0.73768744 -2.30717959  0.44304772 -0.74493354 -2.30717959 -0.61695697\n",
      " -0.45821479]\n",
      "[ 0.74836498 -0.17098353  0.9612148   0.66732794 -0.13303834  0.71301433\n",
      "  0.12012466] [ 0.6719064  -0.2078351   0.92838951  0.62369611 -0.2078351   0.67341875\n",
      "  0.08994834]\n",
      "[ 0.87253006 -1.84069096  0.81276048  0.85019824 -1.81829382  0.85135502\n",
      "  1.58215907] [ 0.82852793 -1.88731069  0.78972042  0.78971744 -1.88731069  0.80609767\n",
      "  1.56285055]\n",
      "[ 1.0645312  -0.34987554  1.31680427  1.02140026 -0.35122631  1.06536958\n",
      " -0.27558776] [ 0.98514947 -0.41776955  1.27506221  0.95852905 -0.41776955  1.01662605\n",
      " -0.2933458 ]\n",
      "[ 0.22596626 -0.33885021  0.57446183  0.18123445 -0.33586988  0.25367687\n",
      " -0.31659204] [ 0.20204178 -0.41776955  0.51238226  0.14237376 -0.41776955  0.18999872\n",
      " -0.34731407]\n",
      "[1.94218647 0.01924954 1.10646398 2.05375997 0.01943831 1.97026297\n",
      " 1.10656966] [1.92487869 0.00209934 1.06705859 2.02999446 0.00209934 1.95584211\n",
      " 1.0413574 ]\n",
      "[ 0.21637508 -0.18659578  1.19088266  0.1825321  -0.16739282  0.33447021\n",
      "  1.12510719] [ 0.20204178 -0.2078351   1.13639313  0.14237376 -0.2078351   0.26533691\n",
      "  1.04153762]\n",
      "[ 0.54426338 -0.78808484  0.91619494  0.54161758 -0.80673749  0.54217548\n",
      "  0.79891153] [ 0.51528486 -0.83763845  0.85905497  0.46046505 -0.83763845  0.51813838\n",
      "  0.73404616]\n",
      "[ 1.21744114 -1.81439661  1.21543773  1.1700985  -1.86684942  1.22566625\n",
      "  0.60457351] [ 1.14177101 -1.88731069  1.13639313  1.13013093 -1.88731069  1.15432751\n",
      "  0.56100144]\n",
      "[ 0.24266734 -1.2428448   0.80725435  0.20152673 -1.2314693   0.25154114\n",
      " -0.13813683] [ 0.20204178 -1.25750735  0.72038588  0.14237376 -1.25750735  0.21511145\n",
      " -0.15653725]\n",
      "[0.70008542 0.25157388 1.41690924 0.70649487 0.23474458 0.75122147\n",
      " 0.59698185] [0.6719064  0.21203379 1.34439675 0.62369611 0.21203379 0.72364421\n",
      " 0.5131026 ]\n",
      "[ 1.0575446  -0.59879143  0.86211322  0.96955809 -0.5848486   1.04727605\n",
      " -0.16429186] [ 0.98514947 -0.627704    0.78972042  0.95852905 -0.627704    0.95802968\n",
      " -0.20345648]\n",
      "[ 1.33736283 -1.18376111  1.4693927   1.39035531 -1.19199657  1.40024303\n",
      "  1.92160065] [ 1.29839254 -1.25750735  1.4137313   1.30452309 -1.25750735  1.3447657\n",
      "  1.88555113]\n",
      "[ 1.00388068 -0.54552545  1.331328    1.04824857 -0.57822514  1.04613516\n",
      "  2.31751761] [ 0.98514947 -0.627704    1.27506221  0.95852905 -0.627704    1.01662605\n",
      "  2.24724411]\n",
      "[ 1.48253904 -0.3360778   1.03061165  1.53643852 -0.38537491  1.49369106\n",
      "  0.16903831] [ 1.45501408 -0.41776955  0.99772405  1.48170552 -0.41776955  1.45400607\n",
      "  0.09493325]\n",
      "[ 1.79665801 -0.14867418  1.13668539  1.9028084  -0.16828579  1.85750975\n",
      "  0.12639609] [ 1.76825716 -0.2078351   1.06705859  1.84444121 -0.2078351   1.78884246\n",
      "  0.11498483]\n",
      "[ 2.13031193 -0.61475176  1.39634861  2.28295702 -0.55609264  2.23758793\n",
      "  1.96023561] [ 2.08150023 -0.627704    1.34439675  2.21833799 -0.627704    2.15883666\n",
      "  1.94105101]\n",
      "[ 1.93870135 -0.18066672  1.56041281  2.11801441 -0.12875525  2.08532673\n",
      "  1.42760393] [ 1.92487869 -0.2078351   1.48306584  2.02999446 -0.2078351   2.00606756\n",
      "  1.36780975]\n",
      "[ 0.91151934 -0.33192591  1.18529208  0.8273386  -0.3295012   0.92023058\n",
      "  0.92285326] [ 0.82852793 -0.41776955  1.13639313  0.78971744 -0.41776955  0.84795222\n",
      "  0.87421373]\n",
      "[ 0.43985603 -0.976598    0.46979883  0.37194365 -0.95853729  0.39427963\n",
      " -0.4950563 ] [ 0.35866332 -1.0475729   0.44304772  0.30002427 -1.0475729   0.32351473\n",
      " -0.5474978 ]\n",
      "[ 0.06641366 -1.37912983  0.66178995  0.0110565  -1.414533    0.10208473\n",
      " -0.26853482] [ 0.04542025 -1.4674418   0.65105134 -0.01248648 -1.4674418   0.0673649\n",
      " -0.3302076 ]\n",
      "[ 0.10024067 -1.40044235  0.62962672  0.00325017 -1.44736663  0.12380038\n",
      "  0.18551891] [ 0.04542025 -1.4674418   0.5817168  -0.01248648 -1.4674418   0.05899399\n",
      "  0.13804358]\n",
      "[ 0.55261244 -0.75023929  0.77593334  0.50049501 -0.75189489  0.56884218\n",
      "  1.31732969] [ 0.51528486 -0.83763845  0.72038588  0.46046505 -0.83763845  0.50139656\n",
      "  1.28012377]\n",
      "[ 0.87264489 -0.77434589  1.64239355  0.82694243 -0.8000695   0.97017978\n",
      "  1.99534965] [ 0.82852793 -0.83763845  1.55240038  0.78971744 -0.83763845  0.89817767\n",
      "  1.94454083]\n",
      "[-0.07067263 -0.12290878  1.20599737 -0.07460386 -0.1265855   0.02590233\n",
      "  0.16062667] [-0.11120129 -0.2078351   1.13639313 -0.16455644 -0.2078351  -0.01090311\n",
      "  0.07381134]\n",
      "[0.8537187  0.70035046 1.17213499 0.87000378 0.69396365 0.88638149\n",
      " 1.0483689 ] [0.82852793 0.63190269 1.13639313 0.78971744 0.63190269 0.84795222\n",
      " 1.02133893]\n",
      "[ 1.9419092   0.08614647  1.31093964  2.07820865  0.03084898  2.01841807\n",
      " -0.057202  ] [ 1.92487869  0.00209934  1.27506221  2.02999446  0.00209934  1.98095483\n",
      " -0.09909819]\n",
      "[ 1.34190307 -1.63156869  1.08796624  1.35849138 -1.593959    1.3743197\n",
      "  0.98292365] [ 1.29839254 -1.67737625  1.06705859  1.30452309 -1.67737625  1.30291115\n",
      "  0.9307629 ]\n",
      "[ 0.25493795 -0.12898971  0.9092088   0.19244431 -0.17994087  0.28598411\n",
      "  0.07055447] [ 0.20204178 -0.2078351   0.85905497  0.14237376 -0.2078351   0.23185327\n",
      " -0.01875666]\n",
      "[ 0.07959251 -1.20575706  0.85647194  0.02104689 -1.22982839  0.16245498\n",
      " -0.24951059] [ 0.04542025 -1.25750735  0.78972042 -0.01248648 -1.25750735  0.08410671\n",
      " -0.31966474]\n",
      "[ 0.09656018 -1.00840833  1.1209163   0.07558903 -0.98845667  0.14997315\n",
      "  0.70163108] [ 0.04542025 -1.0475729   1.06705859 -0.01248648 -1.0475729   0.11759035\n",
      "  0.63341909]\n",
      "[ 1.03588084 -0.19705439  1.25261339  0.9992497  -0.15953218  1.09268671\n",
      "  0.50339984] [ 0.98514947 -0.2078351   1.20572767  0.95852905 -0.2078351   1.00825514\n",
      "  0.47544708]\n",
      "[ 0.52677139 -0.97188069  0.8009012   0.52934965 -0.95977313  0.55622298\n",
      "  0.27078177] [ 0.51528486 -1.0475729   0.78972042  0.46046505 -1.0475729   0.50976747\n",
      "  0.20230709]\n",
      "[-0.68223742 -1.66104526  0.36977137 -0.67315759 -1.66013826 -0.59217592\n",
      "  1.3897798 ] [-0.73768744 -1.67737625  0.30437864 -0.74493354 -1.67737625 -0.63369879\n",
      "  1.3635154 ]\n",
      "[ 0.21807229 -0.75889919  1.01595156  0.21326106 -0.78052321  0.26600707\n",
      "  1.32097004] [ 0.20204178 -0.83763845  0.92838951  0.14237376 -0.83763845  0.24022418\n",
      "  1.2549275 ]\n",
      "[ 0.42102898 -0.18386097  1.01568226  0.31287639 -0.12405124  0.45301693\n",
      "  1.76682188] [ 0.35866332 -0.2078351   0.92838951  0.30002427 -0.2078351   0.3821111\n",
      "  1.71647571]\n",
      "[ 0.38502398 -0.39155873  0.99817295  0.3515648  -0.37113911  0.46429028\n",
      "  1.8689029 ] [ 0.35866332 -0.41776955  0.92838951  0.30002427 -0.41776955  0.3821111\n",
      "  1.84126905]\n",
      "[ 1.22609496 -0.34756306  1.06063119  1.19309004 -0.36361161  1.22543789\n",
      "  0.03872431] [ 1.14177101 -0.41776955  0.99772405  1.13013093 -0.41776955  1.13758569\n",
      "  0.01985967]\n",
      "[-0.55511677 -1.20897797  0.12638511 -0.55974593 -1.17512983 -0.50202159\n",
      "  0.62540413] [-0.5810659  -1.25750735  0.09637501 -0.60402468 -1.25750735 -0.53199223\n",
      "  0.5905822 ]\n",
      "[ 0.41095167 -0.57298577  0.91770194  0.31277411 -0.61036392  0.45731762\n",
      " -0.00160163] [ 0.35866332 -0.627704    0.85905497  0.30002427 -0.627704    0.37374019\n",
      " -0.04536372]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#print(X)\n",
    "X_=[]\n",
    "for i in range(len(X)):\n",
    "    emp=[]\n",
    "    for j in range(7):\n",
    "        emp.append(X[i][j]+random.uniform(0.01, 0.09))\n",
    "    X_.append(emp)\n",
    "X_=np.array(X_)\n",
    "for i in range(len(X_)):\n",
    "    print(X_[i],X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_accuracy_edited =  0.35\n",
      "coef :- [[ 0.00030509 -0.0002559   0.00039144  0.00030828 -0.00025761  0.00032197\n",
      "   0.00025669]]\n"
     ]
    }
   ],
   "source": [
    "X_train_,X_test_,y_train,y_test=train_test_split(X_,Y,test_size=0.2)  #adding error \n",
    "\n",
    "best_model.fit(X_train_,y_train)\n",
    "y_pred_=best_model.predict(X_test_)\n",
    "best_model_accuracy_edited=accuracy_score(y_test, y_pred_)\n",
    "print(\"best_model_accuracy_edited = \",best_model_accuracy_edited)\n",
    "weight_1=best_model.coef_\n",
    "print(\"coef :-\",weight_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different in accuracy =  -0.65\n"
     ]
    }
   ],
   "source": [
    "print(\"different in accuracy = \",best_model_accuracy_edited-best_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5 Checking deviations in metric and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute change in weight =  [[ 1.56360504e-05 -2.72696547e-05  9.82976138e-07  1.01437788e-05\n",
      "  -2.55573028e-05  1.19916094e-05 -1.38155766e-05]]\n",
      "[array([4.8752295]), array([9.63010668]), array([0.2504906]), array([3.18562197]), array([9.02540041]), array([3.59070926]), array([5.68840211])]\n",
      "Top features are =  ['y', '2*y', 'w', 'x']\n",
      "with change in weight with percentage = [array([9.63010668]), array([9.02540041]), array([5.68840211]), array([4.8752295])]\n"
     ]
    }
   ],
   "source": [
    "change=weight-weight_1\n",
    "print(\"Absolute change in weight = \",change)\n",
    "\n",
    "per_chan=[]\n",
    "for i in range(7):\n",
    "    per_chan.append(abs(change[:,i]/weight[:,i])*100)\n",
    "\n",
    "print(per_chan)\n",
    "\n",
    "top=[]\n",
    "idx=[]\n",
    "top_fe=[]\n",
    "fe= ylabel.copy()\n",
    "for i in range(4):\n",
    "    idx.append(per_chan.index(max(per_chan)))\n",
    "    top.append(max(per_chan))\n",
    "    per_chan.pop(idx[i])\n",
    "    top_fe.append(fe[idx[i]])\n",
    "    fe.pop(idx[i])\n",
    "   \n",
    "    \n",
    "print(\"Top features are = \",top_fe)\n",
    "print(\"with change in weight with percentage =\",list(top))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:red'> According to logistic regression \"y,2y,w,z\" are the most important features by weight of that features. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we start LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameter =  {'C': 1e-05}\n",
      "best score =  1.0\n",
      "best estimator =  LinearSVC(C=1e-05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "parameters = {'C':(np.logspace(-5,2))}\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "lr_svc=LinearSVC()\n",
    "lr_svc_cv = GridSearchCV(lr_svc, parameters,n_jobs=-1,return_train_score=True,cv=cv)\n",
    "lr_svc_cv.fit(X, Y)                                                   # fitting on whole data for find out best hyperparameter \n",
    "\n",
    "\n",
    "\n",
    "print(\"best parameter = \",lr_svc_cv.best_params_)\n",
    "print(\"best score = \",lr_svc_cv.best_score_)\n",
    "print(\"best estimator = \",lr_svc_cv.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef :- [[ 0.00032072 -0.00028317  0.00039242  0.00031842 -0.00028317  0.00033396\n",
      "   0.00024287]]\n",
      "accuracy =  0.45\n"
     ]
    }
   ],
   "source": [
    "best_model_1=LinearSVC(C=1e-05)\n",
    "\n",
    "best_model_1.fit(X_train,y_train)             #fitting training data on best_hyperparameter_model \n",
    "y_pred_0=best_model_1.predict(X_test)\n",
    "best_model_accuracy_1=accuracy_score(y_test, y_pred_0)\n",
    "\n",
    "weight_01=best_model.coef_\n",
    "print(\"coef :-\",weight)\n",
    "print(\"accuracy = \",best_model_accuracy_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_accuracy_edited =  1.0\n",
      "coef :- [[ 0.00030509 -0.0002559   0.00039144  0.00030828 -0.00025761  0.00032197\n",
      "   0.00025669]]\n"
     ]
    }
   ],
   "source": [
    "best_model_1.fit(X_train_,y_train)\n",
    "y_pred_1=best_model_1.predict(X_test_)\n",
    "best_model_accuracy_edited_1=accuracy_score(y_test, y_pred_1)\n",
    "print(\"best_model_accuracy_edited = \",best_model_accuracy_edited_1)\n",
    "weight_12=best_model_1.coef_\n",
    "print(\"coef :-\",weight_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "different in accuracy =  0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"different in accuracy = \",best_model_accuracy_edited_1-best_model_accuracy_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5 Checking deviations in metric and weights with updated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute change in weight =  [[-0.00090117  0.0007787  -0.00116603 -0.00090926  0.0007841  -0.00095067\n",
      "  -0.00076046]]\n",
      "[array([280.97800721]), array([274.99181594]), array([297.13726263]), array([285.54912561]), array([276.89836367]), array([284.66444331]), array([313.11065474])]\n",
      "Top features are =  ['w', 'z', 'x*x', '2*z+3*x*x']\n",
      "with change in weight with percentage = [array([313.11065474]), array([297.13726263]), array([285.54912561]), array([284.66444331])]\n"
     ]
    }
   ],
   "source": [
    "change=weight_01-weight_12\n",
    "print(\"Absolute change in weight = \",change)\n",
    "\n",
    "per_chan=[]\n",
    "for i in range(7):\n",
    "    per_chan.append(abs(change[:,i]/weight[:,i])*100)\n",
    "\n",
    "print(per_chan)\n",
    "\n",
    "top=[]\n",
    "idx=[]\n",
    "top_fe=[]\n",
    "fe1=ylabel.copy()\n",
    "for i in range(4):\n",
    "    idx.append(per_chan.index(max(per_chan)))\n",
    "    top.append(max(per_chan))\n",
    "    per_chan.pop(idx[i])\n",
    "    top_fe.append(fe1[idx[i]])\n",
    "    fe1.pop(idx[i])\n",
    "    \n",
    "print(\"Top features are = \",top_fe)\n",
    "print(\"with change in weight with percentage =\",list(top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:red'>According to Linear SVM \"w,z,x*x,2z+3x*x\" are the most important features by weights and we can see that the small change in the data lead to huge change in weight of the features almost 3 time Therefore we can conclude that linear SVM is more sensitive to the changes in data or having high variance. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "8D_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
